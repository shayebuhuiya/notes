## 一、网络系统

### 1、DMA技术

#### 1.1、为什么要有DMA技术？

在没有 DMA 技术前，I/O 的过程是这样的：

1. CPU 发出对应的指令给**磁盘控制器**，然后返回；
2. 磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到**磁盘控制器的内部缓冲区中**，然后产生一个**中断**；
3. CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进**自己的寄存器**，然后**再把寄存器里的数据写入到内存**，而在**数据传输的期间 CPU 是无法执行其他任务的**。

CPU需要亲自参与数据搬运，这在用千兆网卡或者硬盘传输大量数据时，难以承受。采用DMA直接内存访问优化这个问题：**在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务**。

#### 1.2、DMA流程

1. 用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的内存缓冲区中，进程进入阻塞状态；
2. 操作系统收到请求后，进一步将 I/O 请求发送 DMA，然后让 CPU 执行其他任务；
3. DMA 进一步将 I/O 请求发送给磁盘；
4. 磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满；
5. **DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区（Page Cache）中，此时不占用 CPU，CPU 可以执行其他任务**；
6. 当 DMA 读取了足够多的数据，就会发送中断信号给 CPU；
7. CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回；

**CPU 不再参与「将数据从磁盘控制器缓冲区搬运到内核空间」的工作，这部分工作全程由 DMA 完成**。 CPU 只负责告知DMA控制器传输什么数据，传输的起点终点。

### 2、传统的文件传输性能 - DMA

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E4%BC%A0%E7%BB%9F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.png" alt="img" style="zoom: 50%;" />

- **4次用户态与内核态的上下文切换**。
- **2次系统调用**，一次是read()，一次是write()，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。
- **4 次数据拷贝**，其中**两次是 DMA 的拷贝**，另外**两次则是通过 CPU 拷贝**。这在高并发场景下是严重影响系统性能的。例如服务器发送数据包的任务中：
  - DMA把磁盘数据拷贝到内核的数据缓冲区。
  - CPU把内核缓冲区数据搬到用户缓冲区。
  - CPU把用户的缓冲区数据再拷贝到内核的socket缓冲区。
  - DMA把socket缓冲区数据，拷贝到网卡缓冲区中。

传统的文件传输方式，存在冗余的上下文切换和数据拷贝，在高并发系统里严重影响性能。**要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数。**

- 减少用户态和内核态上下文切换：减少系统调用的次数。
- 减少数据拷贝次数：在文件传输的场景中，因为用户空间不会再对数据再加工，用户的缓冲区没有必要存在。

### 3、零拷贝

#### 3.1、mmap（替换read） + write ：减少一次数据拷贝

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/mmap%20%2B%20write%20%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" alt="img" style="zoom:50%;" />

可以用 `mmap()` 替换 `read()` 系统调用函数。`mmap()` 系统调用函数会**直接把内核缓冲区里的数据「映射」到用户空间**，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。

- 应用进程调用了 `mmap()` 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，**应用进程跟操作系统内核「共享」这个缓冲区**；（将内核拷贝到用户缓存区的过程省掉了）
- 应用进程再调用 `write()`，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据；
- 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。

使用`mmap()` 替换 `read()`会**减少一次数据拷贝**，但仍需要**2次系统调用，4次上下文切换**。

#### 3.2、sendfile - 减少1次拷贝，1次系统调用

 Linux  2.1 提供了一个**专门发送文件的系统调用**函数 `sendfile()`。可以**直接把内核缓冲区的数据拷贝到socket缓冲区**，跳过了内核缓冲区拷贝到用户缓冲区的拷贝。

```c
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。
```

共进行**1次系统调用，2次上下文切换，3次数据拷贝**。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-3%E6%AC%A1%E6%8B%B7%E8%B4%9D.png" alt="img" style="zoom:50%;" />

#### 3.3、SG-DMA（分散-聚集式直接存储访问）真正的零拷贝，节省了CPU整理分散数据的开销

> 为什么叫做分散-聚集？
>
> 普通的DMA技术要求源地址和目标地址必须是连续的，但是SG-DMA的源地址可以是多个分散的块，在目标地址再聚集成一块。
>
> 事实上，从磁盘中读取的数据往往也都是分散的一段一段的，这些分散块需要CPU来整理，但是SG-DMA控制器通过**描述符**，记录数据传输的来源、目标、长度等信息，替代了CPU的工作。

如果网卡支持 SG-DMA技术（scatter-gather 特性），可以**进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区**的过程。

从 Linux 内核 `2.4` 版本开始起，对于支持网卡支持 SG-DMA 技术的情况下， `sendfile()` 系统调用的过程发生了一些变化：

- 第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里；
- 第二步，缓冲区描述符和数据长度传到 socket 缓冲区，这样**网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区**里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就**减少了一次数据拷贝**；

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" alt="img" style="zoom:50%;" />

这就是**零拷贝（Zero-copy）技术**，因为我们**没有在内存层面去拷贝数据**，全程**没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的**。只需要 **2 次上下文切换**、**2次数据拷贝次数**，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU。总体来看，零拷贝技术可以把文件传输的性能提高至少一倍以上。

### 4、大文件传输 - 不使用零拷贝

零拷贝中使用的**内核缓冲区，其实就是Page Cache磁盘高速缓存（Linux的页缓存机制）**。Page Cache有两个功能：**缓存最近被访问的数据；预读功能**；

但是在传输大文件时，Page Cache会不起作用，很快被大文件占满，带来两个问题：

- Page Cache被大文件长期占据，导致**热点小文件**不能使用Page Cache。
- 大文件白白进入一次Page Cache，却因为文件太大，很快被替换出来，大多数大文件页面没有真的使用到Page Cache的便利。

因此**大文件传输不该用Page Cache，不该使用零拷贝。**而采用**异步I/O+直接I/O**替代零拷贝技术。

#### 4.1、异步I/O（不阻塞应用程序）+ 直接I/O（不使用Page Cache，直接到进程缓冲区）

在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I/O + 直接 I/O」来替代零拷贝技术。

**同步I/O阻塞应用程序**的`read`操作：

- 当调用 read 方法时阻塞，内核会向磁盘发起 I/O 请求，磁盘收到请求后，便会寻址，当磁盘数据准备好后，就会向内核发起 I/O 中断，告知内核磁盘数据已经准备好；
- 内核收到 I/O 中断后，就将数据**从磁盘控制器缓冲区拷贝到 PageCache** 里；
- 最后，内核再**把 PageCache 中的数据拷贝到用户缓冲区**，于是 read 调用就正常返回了。

**异步I/O**不会阻塞应用程序的`read`操作，并且通过**直接I/O跳过了Page Cache**，流程是：

- 前半部分，内核向磁盘发起读请求，但是可以**不等待数据就位就可以返回**，于是进程此时可以处理其他任务；
- 后半部分，当内核将磁盘中的数据拷贝到进程缓冲区后，进程将接收到内核的**通知**，再去处理数据；

**使用异步I/O就意味着跳过Page Cache**，这就是**直接I/O**，相反，使用Page Cache的是缓存I/O。

##### 4.1.1、直接 I/O 的应用场景

- 应用程序**已经实现了磁盘数据的缓存**，那么可以**不需要 PageCache 再次缓存**，减少额外的性能损耗。在 MySQL 数据库中，可以通过参数设置开启直接 I/O，默认是不开启；
- 传输大文件的时候，由于**大文件难以命中 PageCache 缓存**，而且会占满 PageCache 导致「热点」文件无法充分利用缓存，从而增大了性能开销，因此，这时应该使用直接 I/O。

##### 4.1.2、直接I/O的缺点

另外，由于**直接 I/O 绕过了 PageCache**，就**无法享受内核的这两点的优化**：

- **合并I/O请求**。内核的 I/O 调度算法会缓存尽可能多的 I/O 请求在 PageCache 中，最后「**合并**」成一个更大的 I/O 请求再发给磁盘，减少磁盘的寻址操作；
- **预读**。内核也会「**预读**」后续的 I/O 请求放在 PageCache 中，减少对磁盘的操作；

传输文件的时候，要根据文件的大小来使用不同的方式：

- 传输**大文件**的时候，使用**「异步 I/O + 直接 I/O」**；
- 传输**小文件**的时候，则使用**「零拷贝技术」**；

#### 4.2、阻塞/非阻塞I/O区分

阻塞等待的是**内核数据准备好、数据从内核态拷贝到用户态**两过程。

- 阻塞I/O：在数据未准备好的情况下，线程阻塞，直到数据准备好。
- 非阻塞I/O：在数据未准备好时，立即返回，继续往下执行，让应用程序不断轮询内核。

#### 4.3、同步/异步I/O区分

- 同步I/O：普通的系统调用read、send都是阻塞I/O，因为最后数据准备好时，从内核态拷贝到用户缓存区的过程，都是同步调用，需要等待这个过程完成。Reactor就是一个**非阻塞、同步网络模型**。
- 异步I/O：aio_read是异步I/O，发起调用后会立即返回，内核自动将数据从内核拷贝到用户缓存。应用程序不需要主动发起拷贝操作。Proactor就是一个异步网络模型。

### 5、I/O多路复用

C10K 就是单机同时处理 1 万个请求的问题，真正实现 C10K 的服务器，要考虑的地方在于服务器的网络 I/O 模型，效率低的模型，会加重系统开销，从而会离 C10K 的目标越来越远。从传统的阻塞型TCP通信可以进行非阻塞优化：

- **阻塞I/O模型：**最基础的TCP的Socket编程是阻塞I/O模型，阻塞I/O会在数据没有准备好时让CPU陷入阻塞，效率很低。
- **非阻塞I/O模型**：客户端在查询到数据没有准备好时，服务端立即返回，这需要客户端不断轮询。

按照连接时是否复用进程、线程可以分为：

- **多进程、多线程方式**：比较传统的方式是多进程、多线程模型。把每个客户端连接分配给一个进程、线程。但是上下文切换的的开销和进程、线程的开销都不小。
- **I/O多路复用模型**：一个进程里可以处理多个文件的I/O。

#### 5.1、多进程模型

##### 5.1.1、实现方式

服务器的主进程负责监听客户的连接，一旦与客户端连接完成，accept() 函数就会返回一个「已连接 Socket」，这时就通过 `fork()` 函数创建一个子进程，实际上就把父进程所有相关的东西都**复制**一份，包括文件描述符、内存地址空间、程序计数器、执行的代码等。

子进程会**复制父进程的文件描述符**，于是就可以**直接使用「已连接 Socket 」**和客户端通信了，子进程只关心已连接Socket，父进程只关心监听Socket。

##### 5.1.2、缺点

- 进程**占据系统资源**，且上下文切换很慢，能同时支持的连接很有限。
- 子进程退出时，必须做好回收工作，避免**子进程变成僵尸进程**（比父进程先结束的子进程，却没有被父进程正确回收，等到父进程回收时，子进程会成为孤儿进程交给init接管），逐渐耗尽系统资源。

#### 5.2、多线程模型

##### 5.2.1、实现方式

当服务器与客户端 TCP 完成连接后，通过 `pthread_create()` 函数创建线程，然后**将「已连接 Socket」的文件描述符传递给线程函数**，接着在线程里和客户端进行通信，从而达到并发处理的目的。

可以使用**线程池**的方式来避免线程的频繁创建和销毁，提前创建若干个线程，这样当由新连接建立时，将这个已连接的 Socket 放入到一个队列里，然后线程池里的线程负责从队列中取出「已连接 Socket 」进行处理。多线程的队列需要加锁，避免多线程竞争。

##### 5.2.2、缺点

多线程模型也无法达到C10K（支持并发一万个连接）。

#### 5.3、select 和 poll （I/O多路复用）

使用**多线程的非阻塞I/O**无法做到高并发的关键是，应用程序不知道什么时候数据准备好了，可以进行下一步操作，就需要不断轮询服务器。虽然非阻塞I/O替代了阻塞方案中数据准备阶段的CPU阻塞，但是还是有大量轮询。因此改进的思路是：希望数据准备好，Socket可用时，主动通知程序。

select/poll/epoll 是内核提供的**I/O多路复用**技术。使用**一个进程来维护多个Socket**，就是多路复用。**进程可以通过一个系统调用函数从内核中获取多个事件**。

在获取事件时，先把所有连接（文件描述符）传给内核，再由内核返回产生了事件的连接，然后在用户态中再处理这些连接对应的请求即可。

##### 5.3.1、**select实现方式：**

- 将**已连接的 Socket 都放到一个文件描述符集合**，然后调用 select 函数将文件描述符集合**拷贝**到内核里。
- 内核通过**遍历文件描述符集合**检查是否有网络事件。**检查到有事件**后，将Socket标记为可读或者可写，再把**整个文件描述符集合拷贝回用户态**。
- **用户态遍历文件描述符集合**找到可读或者可写的Socket，对其进行处理。

需要在内核态、用户态**2次遍历文件描述符集合**，发生**2次拷贝文件描述符集合**。

##### 5.3.2、**select 和 poll方法的区别：**

- select方法使用**固定的位图**表示文件描述符集合，支持的文件描述符受限。并且会受到Linux内核中的 FD_SETSIZE 限制，默认最大值是1024，只能监听 0~1023 的文件描述符。

- poll方法使用动态数组，用**链表**形式组织，突破了select的文件描述符个数限制，但还会受到系统文件描述符限制。


selecte 和 poll本质上区别并不大，都是使用「**线性结构**」存储进程关注的 Socket 集合，因此**都需要遍历文件描述符集合**来找到可读或可写的 Socket，时间复杂度为 O(n)，而且也需要在**用户态与内核态之间拷贝文件描述符集合**，这种方式随着并发数上来，性能的损耗会呈指数级增长。

#### 5.4、epoll

先用epoll_create 创建一个 epoll对象 epfd，再通过 **epoll_ctl** 将**需要监视的 socket** 添加到epfd中，最后调用 **epoll_wait** 等待数据。

- epoll 在内核里使用**红黑树来跟踪进程所有待检测的文件描述字**，把需要监控的 socket 通过 `epoll_ctl()` 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删改一般时间复杂度是 `O(logn)`。**只传入待检测的socket，避免了像select和poll把整个socket集合传入给内核**。
- 使用**事件驱动机制**，维护一个**链表记录就绪事件**。某个socket有事件发生时，通过**回调函数**内核将其加入到就绪事件列表。用户调用`epoll_wait()` 函数时，**只会返回有事件发生的文件描述符**，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。

epoll方式在监听Socket数量多时，效率也不大幅降低。能同时监听的Socket数量上限，是**系统定义的进程打开最大文件描述符个数**。

##### 5.4.1、边缘触发和水平触发

epoll 支持两种事件触发模式，分别是边缘触发（edge-triggered，ET）和水平触发（level-triggered，LT）。

- 使用边缘触发模式（**服务端只苏醒一次**）时，当被监控的 Socket 描述符上有可读事件发生时，**服务器端只会从 epoll_wait 中苏醒一次**，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要**保证一次性将内核缓冲区的数据读取完**。一般配合非阻塞I/O使用，程序一直执行I/O操作。

  边缘触发可以减少 epoll_wait 的系统调用次数，效率更高，但是要保证读取正确。

- 使用水平触发模式（**服务端不断苏醒，直到数据读完**）时，当被监控的 Socket 上有可读事件发生时，**服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束**，目的是告诉我们有数据需要读取；

select/poll 只有水平触发模式，epoll 默认的触发模式是水平触发，但是可以根据应用场景设置为边缘触发模式。

### 6、高性能网络模式-Reactor-非阻塞同步网络模式

普通的I/O 多路复用是面向过程的方式。但是可以基于面向对象的思想，对 I/O 多路复用作了一层封装，让使用者可以忽略底层网络细节——**Reactor 模式**。指的是**来了一个事件，Reactor 就有相对应的反应/响应**。

Reactor 模式也叫 `Dispatcher` 模式，即 **I/O 多路复用监听事件，收到事件后，根据事件类型分配（Dispatch）给某个进程 / 线程**。

Reactor 模式主要由 Reactor 和处理资源池这两个核心部分组成：

- **Reactor** 负责**监听和分发**事件，事件类型包含连接事件、读写事件；
- **处理资源池**负责**处理**事件，如 read -> 业务逻辑 -> send；

#### 6.1、单 Reactor 单进程 / 线程

C 语言实现的是「**单 Reactor 单进程**」的方案，因为 C 语编写完的程序，运行后就是一个独立的进程，不需要在进程中再创建线程。而 Java 语言实现的是「**单 Reactor 单线程**」的方案，因为 Java 程序是跑在 Java 虚拟机这个进程上面的，虚拟机中有很多线程，我们写的 Java 程序只是其中的一个线程而已。

##### 6.1.1、方案结构

<img src="C:\Users\absol\Desktop\面试知识\操作系统\7.网络系统.assets\单Reactor单进程.png" alt="img" style="zoom:50%;" />

进程里有 **Reactor、Acceptor、Handler** 这三个对象：

- Reactor 对象的作用是**监听和分发**事件；
- Acceptor 对象的作用是**获取连接**；
- Handler 对象的作用是**处理业务**；

##### 6.1.2、方案细节

- Reactor 对象通过 select （IO 多路复用接口） **监听事件**，收到事件后**通过 dispatch 进行分发**，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
  - 如果是**连接建立**的事件，则**交由 Acceptor 对象**进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；
  - 如果**不是连接建立**事件， 则交由当前连接对应的 **Handler 对象**来进行响应；

- Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

##### 6.1.3、优缺点

**优点：**单 Reactor 单进程的方案因为**全部工作都在同一个进程内**完成，所以实现起来比较简单，不需要考虑进程间通信，也不用担心多进程竞争。

**缺点：**

- 因为只有一个进程，**无法充分利用 多核 CPU 的性能**；
- Handler 对象在业务处理时，整个进程是无法处理其他连接的事件的，**如果业务处理耗时比较长，那么就造成响应的延迟**；

所以，单 Reactor 单进程的方案**不适用计算机密集型的场景，只适用于业务处理非常快速的场景**。Redis6.0版本之前就是单Reactor单进程模型，因为Redis是在内存上操作的，操作速度很快，性能瓶颈不在CPU上。

#### 6.2、单 Reactor 多线程

由于多进程之间处理并发，共享数据的复杂度比多线程高，**实际中基本是多线程**。克服了「单 Reactor 单线程 / 进程」方案的缺点。

##### 6.2.1、方案结构

<img src="C:\Users\absol\Desktop\面试知识\操作系统\7.网络系统.assets\单Reactor多线程.png" alt="img" style="zoom:50%;" />

建立连接，分发事件的流程，三个工作对象的划分和单进程是一样的，区别在分发后的处理：

- Handler 对象不再负责业务处理，**只负责数据的接收和发送**，Handler 对象通过 read 读取到数据后，会将数据发给**子线程里的 Processor 对象**进行**业务处理**；
- **子线程里的 Processor 对象**就进行业务处理，处理完后，将结果发给主线程中的 Handler 对象，接着由 Handler 通过 send 方法将响应结果发送给 client；

##### 6.2.2、优缺点

**优点：**能够充分利用多核 CPU 的计算能力。

**缺点：**

- **多线程竞争**资源的问题，操作共享资源需要锁。
- **Reactor对象成为性能瓶颈**：一个 Reactor 对象承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方。

#### 6.3、多 Reactor 多线程

主线程只负责监听、接收新连接，子线程进行业务处理。

##### 6.3.1、方案结构

<img src="C:\Users\absol\Desktop\面试知识\操作系统\7.网络系统.assets\主从Reactor多线程.png" alt="img" style="zoom:50%;" />

主线程中有MainRactor对象、Acceptor对象。

子线程有SubReactor对象、Handler对象。

- **主线程**中的 **MainReactor 对象**通过 select 监控连接建立事件，收到事件后通过 **Acceptor 对象**中的 accept 获取连接，将**新的连接分配给某个子线程**；
- **子线程**中的 **SubReactor 对象**将 MainReactor 对象分配的连接加入 select 继续进行监听，并**创建一个 Handler 用于处理连接的响应事件**。
  - 如果有新的事件发生时，SubReactor 对象会调用当前连接对应的 Handler 对象来进行响应。
  - Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

##### 6.3.2、优缺点

Netty 和 Memcache 都采用了「多 Reactor 多线程」的方案。「多 Reactor 多进程」方案的开源软件是 Nginx。

- 主线程和子线程分工明确，**主线程只负责接收新连接，子线程负责完成后续的业务处理**。
- 主线程和子线程的交互很简单，**主线程只需要把新连接传给子线程**，子线程无须返回数据，直接就可以在**子线程将处理结果发送给客户端。**

### 7、异步网络模式-Proactor

异步 I/O 比同步 I/O 性能更好，因为异步 I/O 在「**内核数据准备好**」和「**数据从内核空间拷贝到用户空间**」这两个过程都不用等待。

#### 7.1、Reactor、Proactor对比

- Reactor 是**非阻塞同步**网络模式，感知的是**就绪可读写**事件。在每次感知到有事件发生（比如可读就绪事件）后，就需要**应用进程主动调用 read 方法来完成数据的读取**，也就是要应用进程主动将 socket 接收缓存中的数据读到应用进程内存中，这个**过程是同步的**，读取完数据后应用进程才能处理数据。
- Proactor 是**异步**网络模式， 感知的是**已完成的读写**事件。在发起异步读写请求时，需要**传入数据缓冲区的地址（用来存放结果数据）等信息**，读写工作**全程由操作系统**来做，并不需要像 Reactor 那样还需要应用进程主动发起 read/write 来读写数据，操作系统完成读写工作后，就会**通知应用进程直接处理数据**。

Reactor 可以理解为「来了事件操作系统**通知应用进程，让应用进程来处理**」，而 Proactor 可以理解为「来了事件操作系统来处理，**处理完再通知应用进程**」。

无论是 Reactor，还是 Proactor，都是一种基于「事件分发」的网络编程模式，区别在于 **Reactor 模式是基于「待完成」的 I/O 事件，而 Proactor 模式则是基于「已完成」的 I/O 事件**。

#### 7.2、Proactor性能

在 **Linux 下的异步 I/O 是不完善**的， `aio` 系列函数是由 POSIX 定义的异步操作接口，不是真正的操作系统级别支持的，而是在**用户空间模拟出来的异步**，并且仅仅支持基于本地文件的 aio 异步操作，网络编程中的 socket 是不支持的，这也使得**基于 Linux 的高性能网络程序都是使用 Reactor 方案**。

而 Windows 里实现了一套完整的支持 socket 的异步编程接口，这套接口就是 `IOCP`，是由操作系统级别实现的异步 I/O，真正意义上异步 I/O，因此在 **Windows 里实现高性能网络程序可以使用效率更高的 Proactor 方案**。

### 7、一致性哈希

**负载均衡问题**：大多数网站背后是**多台服务器构成集群**提供对外服务，有多个节点时，要如何**分配客户端请求**的问题。

#### 7.1、加权轮询算法 - 无法应对分布式系统

**轮询**：最简单的方式，引入一个中间的负载均衡层，让它将外界的请求「轮流」的转发给内部的集群。

**加权轮询**：考虑到每个节点的硬件配置有所区别，引入权重值，将硬件配置更好的节点的权重值设高，然后根据各个节点的权重值，按照一定比重分配在不同的节点上，让硬件配置更好的节点承担更多的请求。

加权轮询无法应对分布式系统，**分布式系统中每个节点存储的数据不同**。

#### 7.1、哈希算法

普通的加权轮询算法无法应对「分布式系统（数据分片的系统）」，因为分布式系统中每个节点存储的数据不同。

分布式系统为了提高系统容量，将数据水平切分到不同的节点来存储，但是对于一个分布式KV（key-value） 缓存系统而言，某个key应到哪个节点上获得数据，应当是确定的，因此需要一个**能应对分布式系统的负载均衡算法——哈希算法**。

**哈希算法：**对**节点数量进行取模运算**，但是这样的**缺点**就是：如果节点数量发生了变化，也就是在对系统做扩容或者缩容时，必须**迁移改变了映射关系的数据**，否则会出现查询不到数据的问题。这导致必须进行**数据迁移，成本很高**。最坏情况下所有数据都需要迁移。

#### 7.2、一致性哈希算法 - 不一定保证负载均衡

在分布式存储场景中，数据存储的节点位置需要有确定的映射关系，这就需要哈希算法。但是如果按照节点数量进行取模就会导致变动代价大，因此可以选择模数一致的一致性哈希算法。

**一致性哈希算法**：对一个**固定的值** 2^32 进行取模运算，将取模结果组织成一个圆环——**哈希环**，进行两步哈希，**将「存储节点」和「数据」都映射到一个首尾相连的哈希环上**：一致性哈希需要**2步哈希**：

- 首先，对**存储节点**进行哈希映射，比如根据节点的 IP 地址进行哈希，这一步后得到了存储节点的位置。
- 其次，对**数据**进行哈希映射，对数据进行存储或访问时进行，数据**存储在顺时针碰到的第一个节点**。

**寻址方式**：对指定 key 的值进行读写的时候，要通过 **2 步寻址**：

- 首先，对 key 进行哈希计算，确定此 key 在环上的位置；
- 然后，从这个位置沿着顺时针方向走，遇到的第一节点就是存储 key 的节点。

**优点**：在一致哈希算法中，如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响，**减少了数据迁移量**。

**缺点**：**不均匀和稳定性差**两个问题。一致性哈希算法并**不保证节点能够在哈希环上分布均匀**。极端情况下，会有大量数据的寻址请求集中到某个节点上，负载并不均衡；这时进行扩容与删除节点时，会让大量的数据迁移集中到某节点上，容易导致节点崩溃。

#### 7.3、虚拟节点提高均衡度

如果在哈希环上有大量节点，就会分配更均匀，实际上没有那么多节点时，可以添加**虚拟节点**。

**虚拟节点**：不再将真实节点映射到哈希环上，而是将虚拟节点映射到哈希环上，并将虚拟节点映射到实际节点，所以这里有「两层」映射关系。Nginx 的一致性哈希算法，每个权重为 1 的真实节点就含有160 个虚拟节点。

**优点**：提高节点的均衡度；系统稳定性更高，节点变化时，有不同的节点分担系统性变化。

**虚拟节点结合权重：**使用虚拟节点，还可以为硬件配置更好的节点增加权重，对权重更高的节点增加更多虚拟节点。因此，带虚拟节点的一致性哈希方法**不仅适合硬件配置不同的节点的场景，而且适合节点规模会发生变化的场景**。 

#### 
