## 一、网络技术

### 1、DMA技术

#### 1.1、为什么要有DMA技术？

在没有 DMA 技术前，I/O 的过程是这样的：

1. CPU 发出对应的指令给**磁盘控制器**，然后返回；
2. 磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到**磁盘控制器的内部缓冲区中**，然后产生一个**中断**；
3. CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进**自己的寄存器**，然后**再把寄存器里的数据写入到内存**，而在**数据传输的期间 CPU 是无法执行其他任务的**。

CPU需要亲自参与数据搬运，这在用千兆网卡或者硬盘传输大量数据时，难以承受。采用DMA直接内存访问优化这个问题：**在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务**。

### 2、传统的文件传输性能 - DMA（服务器发送数据包场景）

<img src="C:\Users\absol\Desktop\面试知识\操作系统\操作系统整理.assets\传统文件传输.png" alt="img" style="zoom: 50%;" />

- **4次用户态与内核态的上下文切换**。
- **2次系统调用**，一次是read()，一次是write()，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。
- **4 次数据拷贝**，其中**两次是 DMA 的拷贝**，另外**两次则是通过 CPU 拷贝**。这在高并发场景下是严重影响系统性能的。例如服务器发送数据包的任务中：
  - DMA把磁盘数据拷贝到内核的数据缓冲区。
  - CPU把内核缓冲区数据搬到用户缓冲区。
  - CPU把用户的缓冲区数据再拷贝到内核的socket缓冲区。
  - DMA把socket缓冲区数据，拷贝到网卡缓冲区中。

传统的文件传输方式，存在冗余的上下文切换和数据拷贝，在高并发系统里严重影响性能。**要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数。**

- 减少用户态和内核态上下文切换：减少系统调用的次数。
- 减少数据拷贝次数：在文件传输的场景中，因为用户空间不会再对数据再加工，用户的缓冲区没有必要存在。

### 3、零拷贝

#### 3.1、mmap（替换read） + write ：减少一次数据拷贝

<img src="C:\Users\absol\Desktop\面试知识\操作系统\操作系统整理.assets\mmap %2B write 零拷贝.png" alt="img" style="zoom:50%;" />

可以用 `mmap()` 替换 `read()` 系统调用函数。`mmap()` 系统调用函数会**直接把内核缓冲区里的数据「映射」到用户空间**，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。

- 应用进程调用了 `mmap()` 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，**应用进程跟操作系统内核「共享」这个缓冲区**；（将内核拷贝到用户缓存区的过程省掉了）
- 应用进程再调用 `write()`，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据；
- 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。

使用`mmap()` 替换 `read()`会**减少一次数据拷贝**，但仍需要**2次系统调用，4次上下文切换**。

#### 3.2、sendfile - 减少1次拷贝，1次系统调用

 Linux  2.1 提供了一个**专门发送文件的系统调用**函数 `sendfile()`。可以**直接把内核缓冲区的数据拷贝到socket缓冲区**，跳过了内核缓冲区拷贝到用户缓冲区的拷贝。

```c
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。
```

共进行**1次系统调用，2次上下文切换，3次数据拷贝**。

<img src="C:\Users\absol\Desktop\面试知识\操作系统\操作系统整理.assets\senfile-3次拷贝.png" alt="img" style="zoom:50%;" />

#### 3.3、SG-DMA（分散-聚集式直接存储访问）真正的零拷贝，节省了CPU整理分散数据的开销

> 为什么叫做分散-聚集？
>
> 普通的DMA技术要求源地址和目标地址必须是连续的，但是SG-DMA的源地址可以是多个分散的块，在目标地址再聚集成一块。
>
> 事实上，从磁盘中读取的数据往往也都是分散的一段一段的，这些分散块需要CPU来整理，但是SG-DMA控制器通过**描述符**，记录数据传输的来源、目标、长度等信息，替代了CPU的工作。

如果网卡支持 SG-DMA技术（scatter-gather 特性），可以**进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区**的过程。

从 Linux 内核 `2.4` 版本开始起，对于支持网卡支持 SG-DMA 技术的情况下， `sendfile()` 系统调用的过程发生了一些变化：

- 第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里；
- 第二步，缓冲区描述符和数据长度传到 socket 缓冲区，这样**网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区**里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就**减少了一次数据拷贝**；

<img src="C:\Users\absol\Desktop\面试知识\操作系统\操作系统整理.assets\senfile-零拷贝.png" alt="img" style="zoom:50%;" />

这就是**零拷贝（Zero-copy）技术**，因为我们**没有在内存层面去拷贝数据**，全程**没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的**。只需要 **2 次上下文切换**、**2次数据拷贝次数**，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU。总体来看，零拷贝技术可以把文件传输的性能提高至少一倍以上。

### 4、大文件传输 - 不使用零拷贝

零拷贝中使用的**内核缓冲区，其实就是Page Cache磁盘高速缓存（Linux的页缓存机制）**。Page Cache有两个功能：**缓存最近被访问的数据；预读功能**；

但是在传输大文件时，Page Cache会不起作用，很快被大文件占满，带来两个问题：

- Page Cache被大文件长期占据，导致**热点小文件**不能使用Page Cache。
- 大文件白白进入一次Page Cache，却因为文件太大，很快被替换出来，大多数大文件页面没有真的使用到Page Cache的便利。

因此**大文件传输不该用Page Cache，不该使用零拷贝。**而采用**异步I/O+直接I/O**替代零拷贝技术。

#### 4.1、异步I/O（不阻塞应用程序）+ 直接I/O（不使用Page Cache，直接到进程缓冲区）

在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I/O + 直接 I/O」来替代零拷贝技术。

**同步I/O阻塞应用程序**的`read`操作：

- 当调用 read 方法时阻塞，内核会向磁盘发起 I/O 请求，磁盘收到请求后，便会寻址，当磁盘数据准备好后，就会向内核发起 I/O 中断，告知内核磁盘数据已经准备好；
- 内核收到 I/O 中断后，就将数据**从磁盘控制器缓冲区拷贝到 PageCache** 里；
- 最后，内核再**把 PageCache 中的数据拷贝到用户缓冲区**，于是 read 调用就正常返回了。

**异步I/O**不会阻塞应用程序的`read`操作，并且通过**直接I/O跳过了Page Cache**，流程是：

- 前半部分，内核向磁盘发起读请求，但是可以**不等待数据就位就可以返回**，于是进程此时可以处理其他任务；
- 后半部分，当内核将磁盘中的数据拷贝到进程缓冲区后，进程将接收到内核的**通知**，再去处理数据；

**使用异步I/O就意味着跳过Page Cache**，这就是**直接I/O**，相反，使用Page Cache的是缓存I/O。

##### 4.1.1、直接 I/O 的应用场景

- 应用程序**已经实现了磁盘数据的缓存**，那么可以**不需要 PageCache 再次缓存**，减少额外的性能损耗。在 MySQL 数据库中，可以通过参数设置开启直接 I/O，默认是不开启；
- 传输大文件的时候，由于**大文件难以命中 PageCache 缓存**，而且会占满 PageCache 导致「热点」文件无法充分利用缓存，从而增大了性能开销，因此，这时应该使用直接 I/O。

##### 4.1.2、直接I/O的缺点

另外，由于**直接 I/O 绕过了 PageCache**，就**无法享受内核的这两点的优化**：

- **合并I/O请求**。内核的 I/O 调度算法会缓存尽可能多的 I/O 请求在 PageCache 中，最后「**合并**」成一个更大的 I/O 请求再发给磁盘，减少磁盘的寻址操作；
- **预读**。内核也会「**预读**」后续的 I/O 请求放在 PageCache 中，减少对磁盘的操作；

传输文件的时候，要根据文件的大小来使用不同的方式：

- 传输**大文件**的时候，使用**「异步 I/O + 直接 I/O」**；
- 传输**小文件**的时候，则使用**「零拷贝技术」**；

#### 4.2、阻塞/非阻塞I/O区分

阻塞等待的是**内核数据准备好、数据从内核态拷贝到用户态**两过程。

- 阻塞I/O：在数据未准备好的情况下，线程阻塞，直到数据准备好。
- 非阻塞I/O：在数据未准备好时，立即返回，继续往下执行，让应用程序不断轮询内核。

#### 4.3、同步/异步I/O区分

- 同步I/O：普通的系统调用read、send都是阻塞I/O，因为最后数据准备好时，从内核态拷贝到用户缓存区的过程，都是同步调用，需要等待这个过程完成。Reactor就是一个**非阻塞、同步网络模型**。
- 异步I/O：aio_read是异步I/O，发起调用后会立即返回，内核自动将数据从内核拷贝到用户缓存。应用程序不需要主动发起拷贝操作。Proactor就是一个异步网络模型。

### 5、I/O多路复用

C10K 就是单机同时处理 1 万个请求的问题，真正实现 C10K 的服务器，要考虑的地方在于服务器的网络 I/O 模型，效率低的模型，会加重系统开销，从而会离 C10K 的目标越来越远。从传统的阻塞型TCP通信可以进行非阻塞优化：

- **阻塞I/O模型：**最基础的TCP的Socket编程是阻塞I/O模型，阻塞I/O会在数据没有准备好时让CPU陷入阻塞，效率很低。
- **非阻塞I/O模型**：客户端在查询到数据没有准备好时，服务端立即返回，这需要客户端不断轮询。

按照连接时是否复用进程、线程可以分为：

- **多进程、多线程方式**：比较传统的方式是多进程、多线程模型。把每个客户端连接分配给一个进程、线程。但是上下文切换的的开销和进程、线程的开销都不小。
- **I/O多路复用模型**：一个进程里可以处理多个文件的I/O。
