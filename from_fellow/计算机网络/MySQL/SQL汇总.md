## 一、MySQL的基本架构

1. MySQL的架构分为两层：Server层和存储引擎层。

   ![查询语句执行流程](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/mysql%E6%9F%A5%E8%AF%A2%E6%B5%81%E7%A8%8B.png) 

   - **Server 层负责建立连接、分析和执行 SQL**。MySQL 大多数的核心功能模块都在这实现，主要包括**连接器，查询缓存、解析器、预处理器、优化器、执行器等**。另外，所有的**内置函数**和所有跨存储引擎的功能（如存储过程、触发器、视图等。）都在 Server 层实现。
   - **存储引擎层负责数据的存储和提取**。支持 InnoDB、MyISAM、Memory 等多个存储引擎，不同的存储引擎共用一个 Server 层。现在最常用的存储引擎是 InnoDB，从 MySQL 5.5 版本开始， InnoDB 成为了 MySQL 的默认存储引擎。**索引**就是由存储引擎层实现的。



## 二、连接器

### 1、连接MySQL服务

```shell
# -h 指定 MySQL 服务得 IP 地址，如果是连接本地的 MySQL服务，可以不用这个参数；
# -u 指定用户名，管理员角色名为 root；
# -p 指定密码，如果命令行中不填写密码（为了密码安全，建议不要在命令行写密码），就需要在交互对话里面输入密码
mysql -h$ip -u$user -p
```

- 连接的过程需要**先经过TCP三次握手**。

- 通过用户密码验证后，连接器会获取该用户的权限，保存起来，后续的任何操作都会基于连接开始时读到的权限进行权限逻辑处理。

  所以，如果一个用户已经建立了连接，即使管理员中途修改了该用户的权限，也**不会影响已经存在连接的权限**。修改完成后，**只有再新建的连接才会使用新的权限设置**。 

### 2、查看MySQL服务连接的所有客户端

执行 `show processlist` 命令进行查看。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/%E6%9F%A5%E7%9C%8B%E8%BF%9E%E6%8E%A5.png) 

比如上图的显示结果，共有两个用户名为 root 的用户连接了 MySQL 服务，其中 id 为 6 的用户的 Command 列的状态为 `Sleep` ，这意味着该用户连接完 MySQL 服务就没有再执行过任何命令，也就是说这是一个空闲的连接，并且空闲的时长是 736 秒（ Time 列）。

### 3、空闲连接

Command列为sleep的连接。

> 空闲连接会一直占用吗？
>
> MySQL定义了**空闲连接的最大空闲时长， 由 `wait_timeout`** 参数控制，默认是8小时，如果超过这个时间，连接器就会把空闲连接断开。

#### 3.1、手动断开连接的方式

使用 kill connection + id 的命令手动断开空闲的连接。

#### 3.2、MySQL连接数限制

MySQL 服务支持的**最大连接数由 max_connections 参数控制**，系统就会拒绝接下来的连接请求，并报错提示“Too many connections”。 

#### 3.3、短连接和长连接

MySQL 的连接也跟 HTTP 一样，有短连接和长连接的概念，它们的区别如下：

```c
// 短连接
连接 mysql 服务（TCP 三次握手）
执行sql
断开 mysql 服务（TCP 四次挥手）

// 长连接
连接 mysql 服务（TCP 三次握手）
执行sql
执行sql
执行sql
....
断开 mysql 服务（TCP 四次挥手）
```

**优点：**长连接的好处就是可以**减少建立连接和断开连接的过程**，所以一般是推荐使用长连接。

**缺点：**使用长连接后可能会**占用内存增多**，因为 MySQL 在执行查询过程中临时**使用内存管理连接对象**，这些连接对象资源只有在连接断开时才会释放。这样会发生 MySQL 服务被系统杀掉，异常重启的现象。

##### 3.3.1、解决长连接占用内存的问题

- 第一种，**定期断开长连接**。既然断开连接后就会释放连接占用的内存资源，那么我们可以定期断开长连接。

- 第二种，**客户端主动重置连接**。MySQL 5.7 版本实现了 `mysql_reset_connection()` 函数的接口，注意这是**接口函数不是命令**，那么当客户端执行了一个很大的操作后，在代码里调用 mysql_reset_connection 函数来重置连接，达到释放内存的效果。

  这个过程**不需要重连和重新做权限验证**，但是会**将连接恢复到刚刚创建完时的状态**。

### 4、连接器的工作总结

- 与客户端进行 TCP 三次握手**建立连接**；
- **校验客户端的用户名和密码**，如果用户名或密码不对，则会报错；
- 如果用户名和密码都对了，会**读取该用户的权限**，然后后面的权限逻辑判断都基于此时读取到的权限。



## 三、查询缓存（已停用）

连接器工作完成后，客户端就可以向 MySQL 服务发送 SQL 语句了，MySQL 服务收到 SQL 语句后，就会解析出 SQL 语句的第一个字段，看看是什么类型的语句。

如果 SQL 是查询语句（select 语句），MySQL 就会先去**查询缓存（ Query Cache ）**里查找缓存数据，看看之前有没有执行过这一条命令，这个查询缓存是以 key-value 形式保存在内存中的，key 为 SQL 查询语句，value 为 SQL 语句查询的结果。

**但是MySQL查询缓存命中率太低，已经停用**。因为对于一个更新频繁的表，一旦发生更新就会清楚缓存。

MySQL 8.0 版本**移除的是 server 层的查询缓存**，并不是 Innodb 存储引擎中的 buffer pool。 



## 四、解析器

解析器做两件事：

-  **词法分析**。MySQL 会根据输入的字符串识别出关键字出来，构建出 **SQL 语法树**，这样方便后面模块获取 SQL 类型、表名、字段名、 where 条件等等。 
-  **语法分析**。根据词法分析的结果，语法解析器会根据语法规则，判断你输入的这个 SQL 语句**是否满足 MySQL 语法**。如果输入的SQL语法不对，就会在解析器阶段报错。

检查表、字段是否存在不是由解析器完成的，而是在**执行阶段的预处理**完成的。



## 五、执行SQL

### 1、每条SELECT查询语句的三个阶段

- prepare阶段：预处理
- optimize阶段：优化
- execute阶段：执行

### 2、预处理器

- **检查 SQL 查询语句中的表或者字段是否存在；**
- 将 `select *` 中的 `*` 符号，扩展为表上的所有列；

### 3、优化器

**优化器主要负责将 SQL 查询语句的执行方案确定下来**，比如在表里面有多个索引的时候，优化器会基于查询成本的考虑，来决定选择使用哪个索引。 

> 查看SQL具体使用了怎样的索引的方法：可以在**查询语句前加上explain。** 这样就会输出这条 SQL 语句的执行计划，然后执行计划中的 key 就表示执行过程中使用了哪个索引 。

### 4、执行器

真正执行语句，执行器会与存储引擎交互，**交互以记录为单位**。

执行器与存储引擎有**三种交互方式**：

#### 4.1、主键索引查询

```sql
select * from product where id = 1;
```

这条查询语句的查询条件用到了**主键索引**，而且是等值查询，同时主键 id 是唯一，不会有 id 相同的记录，所以**优化器决定选用访问类型为 const 进行查询**，也就是使用**主键索引**查询一条记录，那么执行器与存储引擎的执行流程是这样的：

- 执行器第一次查询，会调用 read_first_record 函数指针指向的函数，因为**优化器**选择的访问类型为 const，这个函数指针被指向为 **InnoDB 引擎索引查询的接口**，把条件 `id = 1` 交给存储引擎，**让存储引擎定位符合条件的第一条记录**。
- **存储引擎**通过**主键索引的 B+ 树**结构定位到 id = 1的第一条记录，如果记录是不存在的，就会向执行器上报记录找不到的错误，然后查询结束。如果记录是存在的，就会将记录返回给执行器；
- **执行器**从存储引擎读到记录后，接着判断记录是否符合查询条件，如果符合则**发送给客户端**，如果不符合则跳过该记录。
- **执行器查询的过程是一个 while 循环**，所以还会再查一次，但是这次因为不是第一次查询了，所以会调用 read_record 函数指针指向的函数，因为优化器选择的访问类型为 const，这个函数指针被指向为一个永远返回 - 1 的函数，所以当调用该函数的时候，执行器就退出循环，也就是结束查询了。

#### 4.2、全表扫描

```sql
select * from product where name = 'iphone';
```

这条查询语句的查询条件**没有用到索引**，所以优化器决定选用访问类型为 ALL 进行查询，也就是全表扫描的方式查询，那么这时执行器与存储引擎的执行流程是这样的：

- 执行器第一次查询，会调用 read_first_record 函数指针指向的函数，因为**优化器选择的访问类型为 all**，这个函数指针被指向为 **InnoDB 引擎全扫描的接口**，**让存储引擎读取表中的第一条记录**；
- **执行器**会判断读到的这条记录的 name 是不是 iphone，如果不是则跳过；如果是则**将记录发给客户**的（是的没错，**Server 层每从存储引擎读到一条记录就会发送给客户端**，之所以客户端显示的时候是直接显示所有记录的，是因为客户端是等查询语句查询完成后，才会显示出所有的记录）。
- 执行器查询的过程是**一个 while 循环**，所以还会再查一次，会调用 read_record 函数指针指向的函数，因为优化器选择的访问类型为 all，read_record 函数指针指向的还是 InnoDB 引擎全扫描的接口，所以接着**向存储引擎层要求继续读刚才那条记录的下一条记录**，存储引擎把下一条记录取出后就将其返回给执行器（Server层），执行器继续判断条件，不符合查询条件即跳过该记录，否则发送到客户端；
- 一直重复上述过程，**直到存储引擎把表中的所有记录读完**，然后向执行器（Server层） 返回了读取完毕的信息；
- 执行器收到存储引擎报告的查询完毕的信息，退出循环，停止查询。

#### 4.3、索引下推

索引下推能够减少**二级索引**在查询时的回表操作，提高查询的效率，因为它**将 Server 层部分负责的事情，交给存储引擎层去处理**了。

案例：如果对 age 和 reward 字段建立了联合索引（age，reward）：

现在有下面这条查询语句：

```sql
select * from t_user  where age > 20 and reward = 100000;
```

联合索引当遇到范围查询 (>、<) 就会停止匹配，也就是 **age 字段能用到联合索引，但是 reward 字段则无法利用到索引**。

##### 4.3.1、不使用下推索引

- Server 层首先调用存储引擎的接口定位到**满足查询条件的第一条二级索引记录**，也就是定位到 age > 20 的第一条记录；
- **存储引擎**根据**二级索引的 B+ 树**快速定位到这条记录后，获取主键值，然后**进行回表操作**，将完整的记录返回给 Server 层；
- Server 层在**判断该记录的 reward 是否等于 100000**，如果成立则将其发送给客户端；否则跳过该记录；
- 接着，继续向存储引擎索要下一条记录，存储引擎在二级索引定位到记录后，获取主键值，然后**回表操作**，将完整的记录返回给 Server 层；
- 如此往复，直到存储引擎把表中的所有记录读完。

可以看到，**没有索引下推的时候，每查询到一条二级索引记录，都要进行回表操作**，然后将记录返回给 Server，接着 **Server 再判断**该记录的 reward 是否等于 100000。

##### 4.3.2、使用下推索引

- Server 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age > 20 的第一条记录；
- 存储引擎定位到二级索引后，**先不执行回表**操作，而是先判断一下该索引中包含的列（reward列）的条件（reward 是否等于 100000）是否成立。如果**条件不成立**，则直接**跳过该二级索引**。如果**成立**，则**执行回表**操作，将完成记录返回给 Server 层。
- Server 层在判断其他的查询条件（本次查询没有其他条件）是否成立，如果成立则将其发送给客户端；否则跳过该记录，然后向存储引擎索要下一条记录。
- 如此往复，直到存储引擎把表中的所有记录读完。

可以看到，使用了索引下推后，虽然 reward 列无法使用到联合索引，但是因为它包含在联合索引（age，reward）里，所以**直接在存储引擎过滤**出满足 reward = 100000 的记录后，才去执行回表操作获取整个记录。相比于没有使用索引下推，节省了很多回表操作。

当你发现执行计划里的 Extra 部分显示了 “Using index condition”，说明使用了索引下推。



## 六、MySQL的数据存放位置

### 1、数据存放方式

MySQL的数据存放位置和存储引擎相关。以InnoDB为例：

我们每创建一个 database（数据库） 都会在 /var/lib/mysql/ 目录里面创建一个以 database 为名的目录，然后保存表结构和表数据的文件都会存放在这个目录里。

- db.opt，用来存储当前数据库的默认字符集和字符校验规则。
- t_order.frm ，t_order 的**表结构**会保存在这个文件。在 MySQL 中建立一张表都会生成一个**.frm 文件**，该文件是用来保存每个表的元数据信息的，主要包含表结构定义。
- t_order.ibd，t_order 的**表数据**会保存在这个文件。MySQL 中每一张表的数据都存放在一个独立的 **.ibd 文件**。

**一张数据库表的数据是保存在「 表名字.ibd 」的文件里的，这个文件也称为独占表空间文件。**

### 2、表空间文件的结构

**表空间由段（segment）、区（extent）、页（page）、行（row）组成**，InnoDB存储引擎的逻辑存储结构大致如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/row_format/%E8%A1%A8%E7%A9%BA%E9%97%B4%E7%BB%93%E6%9E%84.drawio.png)

#### 2.1、行

数据库表中的**记录都是按行（row）进行存放**的，每行记录根据不同的行格式，有不同的存储结构。

#### 2.2、页

**InnoDB 的数据是按「页」为单位来读写的**，也就是说，当需要读一条记录的时候，并不是将这个行记录从磁盘读出来，而是以页为单位，将其整体读入内存。

**默认每个页的大小为 16KB**，也就是最多能保证 16KB 的连续存储空间。**页是 InnoDB 存储引擎磁盘管理的最小单元**，意味着数据库每次读写都是以 16KB 为单位的，一次最少从磁盘中读取 16K 的内容到内存中，一次最少把内存中的 16K 内容刷新到磁盘中。

页的类型有很多，常见的有数据页、undo 日志页、溢出页等等。数据表中的行记录是用「数据页」来管理的 。

#### 2.3、区

InnoDB 存储引擎是用 B+ 树来组织数据的。

B+ 树中每一层都是通过双向链表连接起来的，如果是以页为单位来分配存储空间，那么链表中相邻的两个页之间的物理位置并不是连续的，可能离得非常远，那么磁盘查询时就会有大量的随机I/O，随机 I/O 是非常慢的。**让链表中相邻的页的物理位置也相邻，这样就可以使用顺序 I/O 。**

在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是**按照区（extent）为单位分配**。每个区的大小为 1MB，对于 16KB 的页来说，连续的 64 个页会被划为一个区，这样就使得**链表中相邻的页的物理位置也相邻，就能使用顺序 I/O 了**。

#### 2.4、段

表空间是由各个段（segment）组成的，段是由多个区（extent）组成的。段一般分为数据段、索引段和回滚段等。

- 索引段：存放 B + 树的**非叶子节点**的区的集合；
- 数据段：存放 B + 树的**叶子节点**的区的集合；
- 回滚段：存放的是回滚数据的区的集合，MVCC 利用了回滚段实现了多版本查询数据。

## 七、InnoDB行格式

行格式（row_format），就是一条记录的存储结构。

InnoDB 提供了 4 种行格式，分别是 Redundant、**Compact、Dynamic和 Compressed** 行格式。

- Redundant 是很古老的行格式了， MySQL 5.0 版本之前用的行格式，现在基本没人用了。
- Compact：由于 Redundant 不是一种紧凑的行格式，所以 MySQL 5.0 之后引入了 Compact 行记录存储方式，**Compact 是一种紧凑的行格式**，设计的初衷就是为了让一个数据页中可以存放更多的行记录，从 MySQL 5.1 版本之后，行格式默认设置成 Compact。
- **Dynamic 和 Compressed 两个都是紧凑的行格式**，它们的行格式都和 Compact 差不多，因为都是基于 Compact 改进一点东西。从 MySQL5.7 版本之后，默认使用 Dynamic 行格式。

### 1、compact行格式

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/row_format/COMPACT.drawio.png)

一条完整的记录分为「记录的额外信息」和「记录的真实数据」两个部分。

#### 1.1、记录的额外信息

记录的额外信息包含 3 个部分：变长字段长度列表、NULL 值列表、记录头信息。

##### 1.1.1、变长字段长度列表（逆序存放，不必须有）

例如，char是定长的，varchar是变长的，因此需要把数据占用的大小存储起来，读取时根据**变长字段长度列表**存储的长度读取信息，其他的TEXT、BLOB等变长字段也是这么实现的。

> 创建表如下：
>
> ```sql
> CREATE TABLE `t_user` (
> `id` int(11) NOT NULL,
> `name` VARCHAR(20) DEFAULT NULL,
> `phone` VARCHAR(20) DEFAULT NULL,
> `age` int(11) DEFAULT NULL,
> PRIMARY KEY (`id`) USING BTREE
> ) ENGINE = InnoDB DEFAULT CHARACTER SET = ascii ROW_FORMAT = COMPACT;
> ```
>
> 现在 t_user 表里有这三条记录：
>
> ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/row_format/t_test.png)
>
> 接下来，我们看看看看这三条记录的行格式中的 「变长字段长度列表」是怎样存储的。
>
> 先来看第一条记录：
>
> - name 列的值为 a，真实数据占用的字节数是 1 字节，十六进制 0x01；
> - phone 列的值为 123，真实数据占用的字节数是 3 字节，十六进制 0x03；
> - age 列和 id 列不是变长字段，所以这里不用管。
>
> 这些变长字段的真实数据占用的字节数会按照列的顺序**逆序存放**，所以「变长字段长度列表」里的内容是「 03 01」，而不是 「01 03」。
>
> **第二条记录**的行格式中，「变长字段长度列表」里的内容是「 04 02」；
>
> **第三条记录**中 phone 列的值是 NULL，**NULL 是不会存放在行格式中记录的真实数据部分里的**，所以「变长字段长度列表」里不需要保存值为 NULL 的变长字段的长度。
>
> ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/row_format/%E5%8F%98%E9%95%BF%E5%AD%97%E6%AE%B5%E9%95%BF%E5%BA%A6%E5%88%97%E8%A1%A82.png)

**为什么「变长字段长度列表」的信息要按照逆序存放？**

**原因一**：「记录头信息」中指向下一个记录的指针，指向的是下一条记录的「记录头信息」和「真实数据」之间的位置，这样的好处是**向左读就是记录头信息，向右读就是真实数据，比较方便。**

**原因二**：「变长字段长度列表」中的信息之所以要逆序存放，是因为这样可以**使得位置靠前的记录的真实数据和数据对应的字段长度信息可以同时在一个 CPU Cache Line 中，这样就可以提高 CPU Cache 的命中率**。

当数据表没**有变长字段**的时候，比如全部都是 int 类型的字段，这时候表里的行格式就**不会有「变长字段长度列表」**了，因为没必要，不如去掉以节省空间。 

##### 1.1.2、NULL值列表（逆序存放，高位补零，不必须有）

表中的某些列可能会存储 NULL 值，如果**把这些 NULL 值都放到记录的真实数据中会比较浪费空间**，所以 Compact 行格式把这些值为 NULL 的列存储到 **NULL值列表**中。

如果存在允许 NULL 值的列，则**每个列对应一个二进制位（bit）**，二进制位按照列的顺序逆序排列。

- 二进制位的值为`1`时，代表该列的值为NULL。
- 二进制位的值为`0`时，代表该列的值不为NULL。

另外，**NULL 值列表必须用整数个字节的位表示（1字节8位）**，如果使用的二进制位个数不足整数个字节，则在字节的**高位补 `0`**。

> ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/row_format/t_test.png)
>
> **第一条记录**，第一条记录所有列都有值，不存在 NULL 值，所以用二进制来表示是：00000**000**（高位补零）；
>
> **第二条记录**，第二条记录 age 列是 NULL 值，所以，对于第二条数据，NULL值列表用十六进制表示是 0x04。
>
> ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/row_format/null%E5%80%BC%E5%88%97%E8%A1%A83.png)
>
> 第三条记录则是：00000**110**。

**必须有NULL值列表吗？**

不一定有，**当数据表的字段都定义成 NOT NULL 的时候，这时候表里的行格式就不会有 NULL 值列表了**。

所以在设计数据库表的时候，通常都是建议将字段设置为 NOT NULL，这样可以至少节省 1 字节的空间（NULL 值列表至少占用 1 字节空间）。

**NULL值列表是固定1字节空间吗？**

不会，如果是9哥字段值是NULL，会创建2字节空间的NULL值列表。

##### 1.1.3、记录头信息

记录头信息中包含的内容很多，几个比较重要的：

- delete_mask ：标识此条数据是否被删除。从这里可以知道，我们执行 **detele 删除记录的时候，并不会真正的删除记录，只是将这个记录的 delete_mask 标记为 1**。
- next_record：下一条记录的位置。从这里可以知道，记录与记录之间是通过链表组织的。在前面我也提到了，**指向的是下一条记录的「记录头信息」和「真实数据」之间的位置，这样的好处是向左读就是记录头信息，向右读就是真实数据，比较方便。**
- record_type：**表示当前记录的类型**，0表示普通记录，1表示B+树非叶子节点记录，2表示最小记录，3表示最大记录

#### 1.2、记录的真实数据

记录真实数据部分除了我们定义的字段，还有三个隐藏字段，分别为：row_id、trx_id、roll_pointer。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/row_format/%E8%AE%B0%E5%BD%95%E7%9A%84%E7%9C%9F%E5%AE%9E%E6%95%B0%E6%8D%AE.png)

- row_id：如果我们建表的时候指定了主键或者唯一约束列，那么就没有 row_id 隐藏字段了。如果**既没有指定主键，又没有唯一约束，那么 InnoDB 就会为记录添加 row_id 隐藏字段**。row_id**不是必需**的，占用 6 个字节。

- trx_id：**事务id**，表示这个数据是由哪个事务生成的。 trx_id是**必需**的，占用 6 个字节。

- roll_pointer：这条记录**上一个版本的指针**。roll_pointer 是**必需**的，占用 7 个字节。

trx_id 和roll_pointer是**MVCC机制**使用的。



## 八、varchar(n) 中 n 最大值是多少？

MySQL 规定除了 TEXT、BLOBs 这种大对象类型之外，其他所有的列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过 65535 个字节。

### 1、单字段情况

如果创建varchar(65535)会报错，产生storage overhead。**因为一行数据的最大字节数 65535，其实是包含「变长字段长度列表」和 「NULL 值列表」所占用的字节数的**。

**NULL 值列表占用的字节**：如果字段允许为NULL的，**会用 1 字节来表示「NULL 值列表」**。

**变长字段列表占用的字节**：

- 条件一：如果变长字段允许存储的最大字节数小于等于 255 字节，就会用 1 字节表示「变长字段长度」；
- 条件二：如果变长字段允许存储的最大字节数大于 255 字节，就会用 2 字节表示「变长字段长度」；

我们这里字段类型是 varchar(65535) ，字符集是 ascii，所以代表着变长字段允许存储的最大字节数是 65535，符合条件二，所以会用 2 字节来表示「变长字段长度」。

**在数据库表只有一个 varchar(n) 字段且字符集是 ascii 的情况下，varchar(n) 中 n 最大值 = 65535 - 2 - 1 = 65532**。

如果创建varchar(65535 - 3 = 65532) 就会成功。

### 2、多字段情况

**如果有多个字段的话，要保证所有字段的长度 + 变长字段字节数列表所占用的字节数 + NULL值列表所占用的字节数 <= 65535**。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/row_format/%E5%A4%9A%E5%AD%97%E6%AE%B5%E7%9A%84%E6%83%85%E5%86%B5.png)

### 3、n 代表的是最多存储的字符数量，并不是字节大小。

要算 varchar(n) 最大能允许存储的字节数，**还要看数据库表的字符集**，因为字符集代表着，1个字符要占用多少字节。

- 比如 ascii 字符集， 1 个字符占用 1 字节，那么 varchar(100) 意味着最大能允许存储 100 字节的数据。varchar(n) 的 n 最大取值就是 65532。

- 在 UTF-8 字符集下，一个字符最多需要三个字节，varchar(n) 的 n 最大取值就是 65532/3 = 21844。

## 九、行溢出后处理

一个页的大小一般是 `16KB`，也就是 `16384字节`，而一个 varchar(n) 类型的列最多可以存储 `65532字节`，一些大对象如 TEXT、BLOB 可能存储更多的数据，这时一个页可能就存不了一条记录。这个时候就会**发生行溢出，多的数据就会存到另外的「溢出页」中**。

### 1、Compact 行格式溢出页处理

如果一个数据页存不了一条记录，**InnoDB 存储引擎会自动将溢出的数据存放到「溢出页」中**。在一般情况下，InnoDB 的数据都是存放在 「数据页」中。但是当发生行溢出时，溢出的数据会存放到「溢出页」中。

当发生行溢出时，在**记录的真实数据处只会保存该列的一部分数据，而把剩余的数据放在「溢出页」中**，然后**真实数据处用 20 字节存储指向溢出页的地址**，从而可以找到剩余数据所在的页。大致如下图所示。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/row_format/%E8%A1%8C%E6%BA%A2%E5%87%BA.png)

### 2.Compressed 和 Dynamic 行格式溢出页处理

Compressed 和 Dynamic 这两个行格式和 Compact 非常类似，主要的区别在于处理行溢出数据时有些区别。

这两种格式采用完全的行溢出方式，记录的真实数据处**不会存储该列的一部分数据，只存储 20 个字节的指针来指向溢出页**。而实际的数据都存储在溢出页中，看起来就像下面这样：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/row_format/%E8%A1%8C%E6%BA%A2%E5%87%BA2.png)

## 十、记录存储问题总结

### 1、MySQL的NULL值怎么存放

MySQL 的 Compact 行格式中会用「**NULL值列表**」来标记值为 NULL 的列，NULL 值并不会存储在行格式中的真实数据部分。

NULL值列表会占用 1 字节空间，当表中所有字段都定义成 NOT NULL，行格式中就不会有 NULL值列表，这样可节省 1 字节的空间。

### 2、MySQL 怎么知道 varchar(n) 实际占用数据的大小？

MySQL 的 Compact 行格式中会用「**变长字段长度列表**」存储变长字段实际占用的数据大小。

### 3、varchar(n) 中 n 最大取值为多少？

一行记录最大能存储 65535 字节的数据，但是这个是**包含「变长字段字节数列表所占用的字节数」和「NULL值列表所占用的字节数」**。所以， 我们在算 varchar(n) 中 n 最大值时，需要减去这两个列表所占用的字节数。

如果一张表只有一个 varchar(n) 字段，且允许为 NULL，字符集为 ascii。varchar(n) 中 n 最大取值为 65532。

计算公式：65535 - 变长字段字节数列表所占用的字节数 - NULL值列表所占用的字节数 = 65535 - 2 - 1 = 65532。

如果有多个字段的话，要保证所有字段的长度 + 变长字段字节数列表所占用的字节数 + NULL值列表所占用的字节数 <= 65535。

### 4、行溢出后如何处理？

如果一个数据页存不了一条记录，InnoDB 存储引擎会自动将溢出的数据存放到「溢出页」中。

**Compact 行格式**针对行溢出的处理是这样的：当发生行溢出时，在记录的真实数据处只会保存该列的一部分数据，而把**剩余的数据放在「溢出页」中，然后真实数据处用 20 字节存储指向溢出页的地址**，从而可以找到剩余数据所在的页。

**Compressed 和 Dynamic 这两种格式**采用**完全的行溢出方式**，记录的真实数据处不会存储该列的一部分数据，**只存储 20 个字节的指针来指向溢出页**。而实际的数据都存储在溢出页中。



## 十一、索引基础

### 1、索引定义

索引就是**数据的目录**。

### 2、索引分类

- 按「数据结构」分类：**B+tree索引、Hash索引、Full-text索引**。
- 按「物理存储」分类：**聚簇索引（主键索引）、二级索引（辅助索引）**。
- 按「字段特性」分类：**主键索引、唯一索引、普通索引、前缀索引**。
- 按「字段个数」分类：**单列索引、联合索引**。

#### 2.1、按数据结构分类

从数据结构的角度来看，MySQL 常见索引有 **B+Tree 索引、HASH 索引、Full-Text 索引**。

InnoDB 是在 MySQL 5.5 之后成为默认的 MySQL 存储引擎，**B+Tree 索引类型也是 MySQL 存储引擎采用最多的索引类型**。

在创建表时，InnoDB 存储引擎会根据不同的场景选择不同的列作为**聚簇索引**：

> 聚簇索引并不是一种单独的索引类型，而是一种**数据存储方式**。  当表有聚簇索引时，它的**行数据实际上存放在索引的叶子页**中，因为无法同时把数据行存放在两个不同的地方，所以**一个表只能有一个聚簇索引。** 
>
> **所有完整的用户记录都存放在聚簇索引的叶子节点。**
>
> 优点：
>
> - 可以把**相关数据保存在一起**，数据访问就更快。
> - 聚簇索引提高了**IO密集型应用**的性能。
> - 使用聚簇索引扫描的查询可以直接使用页节点中的主键值。
>
> 缺点：
>
> - 如果数据全部在内存中将失去优势。
> - **更新聚簇索引列的代价很高**，因为会强制每个被更新的行移动到新位置。
> - 基于聚簇索引的表插入新行或主键被更新导致行移动时，可能导致**页分裂**，表会占用更多磁盘空间。

- 如果有**主键**，默认会使用主键作为聚簇索引的索引键（key）；
- 如果没有主键，就选择**第一个不包含 NULL 值的唯一列**作为聚簇索引的索引键（key）；
- 在上面两个都没有的情况下，InnoDB 将自动生成一个**隐式自增 id 列**作为聚簇索引的索引键（key）；

其它索引都属于**辅助索引**（Secondary Index），也被称为**二级索引或非聚簇索引**。**创建的主键索引和二级索引默认使用的是 B+Tree 索引**。

##### 2.1.1、B+ tree索引的存放方式

**B+Tree 是一种多叉树**，**叶子节点才存放数据，非叶子节点只存放索引**，而且每个节点里的数据是**按主键顺序存放**的。每一层**父节点的索引值都会出现在下层子节点的索引值中**，因此在叶子节点中，包括了所有的索引值信息，并且每一个叶子节点都有两个指针，分别指向下一个叶子节点和上一个叶子节点，形成一个**双向链表**。 

##### 2.1.2、B+ tree主键索引查询

> 比如，我们执行了下面这条查询语句：
>
> ```sql
> select * from product where id= 5;
> ```
>
> ![主键索引 B+Tree](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E7%B4%A2%E5%BC%95/btree.drawio.png)
>
> 这条语句使用了主键索引查询 id 号为 5 的商品。查询过程是这样的，B+Tree 会自顶向下逐层进行查找：
>
> - 将 5 与根节点的索引数据 (1，10，20) 比较，5 在 1 和 10 之间，所以根据 B+Tree的搜索逻辑，找到第二层的索引数据 (1，4，7)；
> - 在第二层的索引数据 (1，4，7)中进行查找，因为 5 在 4 和 7 之间，所以找到第三层的索引数据（4，5，6）；
> - 在叶子节点的索引数据（4，5，6）中进行查找，然后我们找到了索引值为 5 的行数据。

数据库的索引和数据都是存储在硬盘的，进行了 3 次 I/O 操作。

B+Tree 存储千万级的数据只需要 3-4 层高度就可以满足，这意味着从千万级的表查询目标数据最多需要 3-4 次磁盘 I/O，所以**B+Tree 相比于 B 树和二叉树来说，最大的优势在于查询效率很高，因为即使在数据量很大的情况，查询一个数据的磁盘 I/O 依然维持在 3-4次。**

##### 2.1.3、B+ tree二级索引查询

主键索引的 B+Tree 和二级索引的 B+Tree 区别如下：

- **主键索引**的 B+Tree 的叶子节点存放的是**实际数据**，所有完整的用户记录都存放在主键索引的 B+Tree 的叶子节点里；
- **二级索引**的 B+Tree 的叶子节点存放的是**主键值**，而不是实际数据。

> 举例：这里将前面的商品表中的 product_no （商品编码）字段设置为二级索引，那么二级索引的 B+Tree 如下图。
>
> ![二级索引 B+Tree](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E7%B4%A2%E5%BC%95/%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95btree.drawio.png)
>
> 其中**非叶子的 key 值是 product_no（图中橙色部分），叶子节点存储的数据是主键值**（图中绿色部分）。
>
> 如果我用 product_no 二级索引查询商品，如下查询语句：
>
> ```
> select * from product where product_no = '0002';
> ```

**会先检二级索引中的 B+Tree 的索引值（**商品编码，product_no），找到对应的叶子节点，**然后获取主键值，然后再通过主键索引中的 B+Tree 树查询到对应的叶子节点**，然后获取整行数据。

**这个过程叫「回表」，也就是说要查两个 B+Tree 才能查到数据**。

> **二级索引不用回表的特殊情况**：当查询的数据是能在二级索引的 B+Tree 的叶子节点里查询到，这时就不用再查主键索引查，比如下面这条查询语句：
>
> 因为二级索引product_no里存放的就是查询对象：主键id。
>
> ```sql
> select id from product where product_no = '0002';
> ```
>
> **这种在二级索引的 B+Tree 就能查询到结果的过程就叫作「覆盖索引」，也就是只需要查一个 B+Tree 就能找到数据。**

##### 2.1.4、为什么 MySQL InnoDB 选择 B+tree 作为索引的数据结构？

***1、B+Tree vs B Tree***

B+Tree **只在叶子节点存储数据**，而 B 树的非叶子节点也要存储数据，所以 **B+Tree 的单个节点的数据量更小，在相同的磁盘 I/O 次数下，就能查询更多的节点**。

另外，B+Tree 叶子节点采用的是双链表连接，适合 MySQL 中常见的**基于范围的顺序查找**，而 B 树无法做到这一点。

***2、B+Tree vs 二叉树***

对于有 N 个叶子节点的 B+Tree，其**搜索复杂度为`O(logdN)`**，其中 d 表示节点允许的最大子节点个数为 d 个。

在实际的应用当中， d 值是大于100的，这样就保证了，即使数据达到千万级别时，B+Tree 的高度依然维持在 3~4 层左右，也就是说一次数据查询操作只需要做 **3~4 次的磁盘 I/O 操作**就能查询到目标数据。

而二叉树的每个父节点的儿子节点个数只能是 2 个，意味着**其搜索复杂度为 `O(logN)`**，这已经比 B+Tree 高出不少，因此**二叉树检索到目标数据所经历的磁盘 I/O 次数要更多**。

***3、B+Tree vs Hash***

Hash 在做等值查询的时候搜索复杂度为 O(1)。但是 **Hash 表不适合做范围查询**，它**更适合做等值的查询**，这也是 B+Tree 索引要比 Hash 表索引有着更广泛的适用场景的原因。

#### 2.2、按物理存储分类

从物理存储的角度来看，索引分为**聚簇索引（往往是主键索引）**、二级索引（辅助索引）。

- 主键索引的 B+Tree 的叶子节点存放的是**实际数据**，所有完整的用户记录都存放在主键索引的 B+Tree 的叶子节点里；
- 二级索引的 B+Tree 的叶子节点存放的是**主键值**，而不是实际数据。

如果查询的数据能在二级索引里查询的到，那么就不需要回表，这个过程就是**覆盖索引**。

如果查询的数据不在二级索引里，就会先检索二级索引，找到对应的叶子节点，获取到主键值后，然后再检索主键索引，就能查询到数据了，这个过程就是**回表**。

#### 2.3、按字段特性分类

从字段特性的角度来看，索引分为**主键索引、唯一索引、普通索引、前缀索引**。

##### 2.3.1、主键索引

主键索引就是建立在主键字段上的索引，通常在创建表的时候一起创建，一张表最多**只有一个主键索引**，索引列的值**不允许有空值**。

##### 2.3.2、唯一索引

唯一索引建立在 UNIQUE 字段上的索引，一张表可以有**多个唯一索引**，索引列的值**必须唯一**，但是**允许有空值**。

##### 2.3.3、普通索引

普通索引就是建立在普通字段上的索引，**既不要求字段为主键，也不要求字段为 UNIQUE**。

##### 2.3.4、前缀索引

前缀索引是指**对字符类型字段的前几个字符建立的索引，而不是在整个字段上建立的索引**，前缀索引可以建立在字段类型为 char、 varchar、binary、varbinary 的列上。

使用前缀索引的目的是为了**减少索引占用的存储空间，提升查询效率。**

#### 2.4、按字段个数分类

从字段个数的角度来看，索引分为**单列索引、联合索引**（复合索引）。

- 建立在单列上的索引称为单列索引，比如主键索引；
- 建立在多列上的索引称为联合索引；

##### 2.4.1、联合索引

通过将多个字段组合成一个索引，该索引就被称为联合索引。

> 举例：将商品表中的 product_no 和 name 字段组合成联合索引`(product_no, name)`，创建联合索引的方式如下：
>
> ```sql
> CREATE INDEX index_product_no_name ON product(product_no, name);
> ```
>
> 当在联合索引查询数据时，先按 product_no 字段比较，在 product_no 相同的情况下再按 name 字段比较。也就是说，联合索引查询的 B+Tree 是先按 product_no 进行排序，然后再 product_no 相同的情况再按 name 字段排序。
>
> 因此，使用联合索引时，存在**最左匹配原则**，也就是按照最左优先的方式进行索引的匹配，**不遵循最左匹配，联合索引就会失效。**
>
> 比如，如果创建了一个 `(a, b, c)` 联合索引，可以匹配上联合索引的查询：
>
> - where a=1；
> - where a=1 and b=2 and c=3；
> - where a=1 and b=2；
>
> 需要注意的是，因为有**查询优化器**，所以 **a 字段在 where 子句的顺序并不重要**。
>
> 因为不符合最左匹配原则，所以就无法匹配上联合索引，联合索引就会失效:
>
> - where b=2；
> - where c=3；
> - where b=2 and c=3；

只单单使用b、c联合索引无效的原因：a 是全局有序的（1, 2, 2, 3, 4, 5, 6, 7 ,8），而 b 是全局是无序的（12，7，8，2，3，8，10，5，2）。**只有在 a 相同的情况才，b 才是有序的。**因此，直接执行`where b = 2`这种查询条件没有办法利用联合索引的，**利用索引的前提是索引里的 key 是有序的**。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E7%B4%A2%E5%BC%95/%E8%81%94%E5%90%88%E7%B4%A2%E5%BC%95%E6%A1%88%E4%BE%8B.drawio.png)

##### 2.4.2、联合索引查询范围

联合索引有一些特殊情况，**并不是查询过程使用了联合索引查询，就代表联合索引中的所有字段都用到了联合索引进行索引查询**。

**情况一：select * from t_table where a > 1 and b = 2**

**在符合 a > 1 条件的二级索引记录的范围里，b 字段的值是无序的**。

因此，**Q1 这条查询语句只有 a 字段用到了联合索引进行索引查询，而 b 字段并没有使用到联合索引**。

**情况二：select * from t_table where a >= 1 and b = 2**

虽然在符合 a>= 1 条件的二级索引记录的范围里，b 字段的值是「无序」的，**但是对于符合 a = 1 的二级索引记录的范围里，b 字段的值是「有序」的**。当二级索引记录的 a 字段值为 1 时，**可以通过 b = 2 条件减少需要扫描的二级索引记录范围**。也就是说，**从符合 a = 1 and b = 2 条件的第一条记录开始扫描**，而不需要从第一个 a 字段值为 1 的记录开始扫描。

所以，**Q2 这条查询语句 a 和 b 字段都用到了联合索引进行索引查询**。

**情况三：SELECT * FROM t_table WHERE a BETWEEN 2 AND 8 AND b = 2**

不同的数据库对 BETWEEN ... AND 处理方式是有差异的。在 MySQL 中，BETWEEN 包含了 value1 和 value2 边界值，类似于 >= and =<。

**Q3 这条查询语句 a 和 b 字段都用到了联合索引进行索引查询**。

**情况四：SELECT * FROM t_user WHERE name like 'j%' and age = 22**

a 字段可以在联合索引的 B+Tree 中进行索引查询，形成的扫描区间是['j','k')。

虽然在符合前缀为 ‘j’ 的 name 字段的二级索引记录的范围里，age 字段的值是「无序」的，**但是对于符合 name = j 的二级索引记录的范围里，age字段的值是「有序」的**

**Q4 这条查询语句 a 和 b 字段都用到了联合索引进行索引查询**。

综上所示，**联合索引的最左匹配原则，在遇到范围查询（如 >、<）的时候，就会停止匹配，也就是范围查询的字段可以用到联合索引，但是在范围查询字段的后面的字段无法用到联合索引。**

**注意，对于 >=、<=、BETWEEN、like 前缀匹配的范围查询，并不会停止匹配。**

#### 2.5、索引下推

对于联合索引（a, b），在执行 `select * from table where a > 1 and b = 2` 语句的时候，只有 a 字段能用到索引，那在联合索引的 B+Tree 找到第一个满足条件的主键值（ID 为 2）后，还需要判断其他条件是否满足（看 b 是否等于 2）。

- 在 MySQL 5.6 之前，只能从 ID2 （主键值）开始一个个**回表**，存储引擎层交给Server层去判断，到「主键索引」上找出数据行，再对比 b 字段值。
- 而 MySQL 5.6 引入的**索引下推优化**（index condition pushdown)， **可以在联合索引遍历过程中，对联合索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数**。

#### 2.6、索引区分度

越靠前的字段被用于索引过滤的概率越高，实际开发工作中**建立联合索引时，要把区分度大的字段排在前面，这样区分度大的字段越有可能被更多的 SQL 使用到**。

区分度就是某个字段 column 不同值的个数「除以」表的总行数。

如果索引的区分度很小，还不如不要索引。MySQL 一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比（惯用的百分比界线是"30%"）很高的时候，它一般会**忽略索引**，**进行全表扫描**。

#### 2.7、联合索引进行排序

针对针对下面这条 SQL，你怎么通过索引来提高查询效率呢？

```sql
select * from order where status = 1 order by create_time asc
```

这时不能仅仅给status建立索引，这样还会导致后续对create_time进行文件排序。

在 status 和 create_time 列建立联合索引，这样根据 status 筛选后的数据就是按照 create_time 排好序的，避免在文件排序，提高了查询效率。

### 3、什么时候需要/不需要创建索引

#### 3.1、索引的缺点

索引最大的好处是**提高查询速度**，但是索引也是有缺点的，比如：

- 需要占用物理空间，数量越大，**占用空间**越大；
- **创建索引和维护索引**要耗费时间，这种时间随着数据量的增加而增大；
- 会降低表的**增删改的效率**，因为每次增删改索引，B+ 树为了维护索引有序性，都需要进行动态维护。

#### 3.2、什么时候适用索引

- 字段有**唯一性**限制的，比如商品编码；
- 经常用于 **`WHERE` 查询条件**的字段，这样能够提高整个表的查询速度，如果查询条件不是一个字段，可以建立联合索引。
- 经常**用于 `GROUP BY` 和 `ORDER BY` 的字段**，这样在查询的时候就不需要再去做一次排序了，因为我们都已经知道**了建立索引之后在 B+Tree 中的记录都是排序好的**。

#### 3.3、什么时候不需要索引

- `WHERE` 条件，`GROUP BY`，`ORDER BY` 里**用不到的字段**，索引的价值是**快速定位**，如果起不到定位的字段通常是不需要创建索引的，因为索引是会占用物理空间的。
- 字段中存在**大量重复数据，不需要创建索引**，比如性别字段，只有男女，如果数据库表中，男女的记录分布均匀，那么无论搜索哪个值都可能得到一半的数据。
- **表数据太少**的时候，不需要创建索引；
- **经常更新的字段**不用创建索引，比如不要对电商项目的用户余额建立索引，因为索引字段频繁修改，由于**要维护 B+Tree的有序性，那么就需要频繁的重建索引**，这个过程是会影响数据库性能的。

### 4、优化索引

#### 4.1、前缀索引优化

使用某个字段中字符串的**前几个字符建立索引**，是为了减小索引字段大小，可以增加一个索引页中存储的索引值，有效提高索引的查询速度。

不过，前缀索引有一定的局限性，例如：

- order by 就无法使用前缀索引；
- 无法把前缀索引用作覆盖索引

#### 4.2、覆盖索引优化-减少回表

覆盖索引是指 SQL 中 select的所有字段，在索引 B+Tree 的叶子节点上都能找得到的那些索引，从**二级索引中查询得到记录，而不需要通过聚簇索引查询获得，可以避免回表的操作**。

> 举例：
>
> 假设我们只需要查询商品的名称、价格，有什么方式可以避免回表呢？
>
> 我们可以建立一个联合索引，即「商品ID、名称、价格」作为一个联合索引。如果索引中存在这些数据，查询将不会再次检索主键索引，从而避免回表。
>
> 所以，使用覆盖索引的好处就是，**不需要查询出包含整行记录的所有信息**，也就减少了大量的 I/O 操作。

#### 4.3、主键索引优化-自增、减小主键长度

**如果我们使用自增主键**，那么每次插入的新数据就会按顺序添加到当前索引节点的位置，不需要移动已有的数据，当页面写满，就会自动开辟一个新页面。因为每次**插入一条新记录，都是追加操作，不需要重新移动数据**，因此这种插入数据的方法效率非常高。

**如果我们使用非自增主键**，由于每次插入主键的索引值都是随机的，因此每次插入新的数据时，就可能会插入到现有数据页中间的某个位置，这将不得不移动其它数据来满足新数据的插入，甚至需要从一个页面复制数据到另外一个页面，我们通常将这种情况称为**页分裂**。**页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率**。

另外，主键字段的长度不要太大，因为**主键字段长度越小，意味着二级索引的叶子节点越小（二级索引的叶子节点存放的数据是主键值），这样二级索引占用的空间也就越小**。

#### 4.4、索引最好设置为 NOT NULL	

索引列要设置为 NOT NULL 约束。有两个原因：

- 第一原因：**索引列存在 NULL 就会导致优化器在做索引选择的时候更加复杂，更加难以优化**，因为可为 NULL 的列会使索引、索引统计和值比较都更复杂，比如**进行索引统计时，count 会省略值为NULL 的行。**

- 第二个原因：NULL 值是一个没意义的值，但是它会**占用物理空间，所以会带来的存储空间的问题**，因为 InnoDB 存储记录的时候，如果表中存在允许为 NULL 的字段，那么[行格式 (opens new window)](https://xiaolincoding.com/mysql/base/row_format.html#innodb-行格式有哪些)中**至少会用 1 字节空间存储 NULL 值列表**，如下图的紫色部分：

  ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/row_format/COMPACT.drawio.png)

#### 4.5、防止索引失效

发生索引失效的情况：

- 当我们使用**左或者左右模糊匹配**的时候，也就是 `like %xx` 或者 `like %xx%`这两种方式都会造成索引失效；
- 当我们在查询条件中对索引列做了**计算、函数、类型转换**操作，这些情况下都会造成索引失效；
- 联合索引要能正确使用**需要遵循最左匹配原则**，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。
- 在 WHERE 子句中，如果在 **OR** 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。

### 5、总结

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E7%B4%A2%E5%BC%95/%E7%B4%A2%E5%BC%95%E6%80%BB%E7%BB%93.drawio.png)



## 十二、B+树

### 1、InnoDB是如何存储数据的

#### 1.1、数据页

InnoDB 里的 B+ 树中的**每个节点都是一个数据页**。**InnoDB 的数据是按「数据页」为单位来读写的**，也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。

数据库的 I/O 操作的最小单位是页，**InnoDB 数据页的默认大小是 16KB**，意味着数据库每次读写都是以 16KB 为单位的，一次最少从磁盘中读取 16K 的内容到内存中，**一次最少把内存中的 16K 内容刷新到磁盘中**。

![图片](https://cdn.xiaolincoding.com//mysql/other/243b1466779a9e107ae3ef0155604a17.png)



这 7 个部分的作用如下图：

![图片](https://cdn.xiaolincoding.com//mysql/other/fabd6dadd61a0aa342d7107213955a72.png)

在 File Header 中有两个指针，分别指向上一个数据页和下一个数据页，连接起来的页相当于一个双向的链表，如下图所示：

![图片](https://cdn.xiaolincoding.com//mysql/other/557d17e05ce90f18591c2305871af665.png)

采用链表的结构是让数据页之间不需要是物理上的连续的，而是逻辑上的连续。

#### 1.2、页目录

**数据页中的记录按照「主键」顺序组成单向链表**，单向链表的特点就是插入、删除非常方便，但是**检索效率不高**，最差的情况下需要遍历链表上的所有节点才能完成检索。

因此，数据页中有一个**页目录**，起到记录的索引作用。

![图片](https://cdn.xiaolincoding.com//mysql/other/261011d237bec993821aa198b97ae8ce.png)

##### 1.2.1、页目录创建的过程

1. 将所有的记录划分成几个组，这些记录包括最小记录和最大记录，但不包括标记为“已删除”的记录；
2. 每个记录组的最后一条记录就是组内最大的那条记录，并且最后一条记录的头信息中会存储该组一共有多少条记录，作为 n_owned 字段（上图中粉红色字段）
3. 页目录用来存储每组最后一条记录的地址偏移量，这些地址偏移量会按照先后顺序存储起来，每组的地址偏移量也被称之为槽（slot），**每个槽相当于指针指向了不同组的最后一个记录**。

从图可以看到，**页目录就是由多个槽组成的，槽相当于分组记录的索引**。然后，因为记录是按照「主键值」从小到大排序的，所以**我们通过槽查找记录时，可以使用二分法快速定位要查询的记录在哪个槽（哪个记录分组），定位到槽后，再遍历槽内的所有记录，找到对应的记录**，无需从最小记录开始遍历整个页中的记录链表。

##### 1.2.2、记录查询过程

查找主键为 11 的用户记录为例：

- 先二分得出槽中间位是 (0+4)/2=2 ，2号槽里最大的记录为 8。因为 11 > 8，所以需要从 2 号槽后继续搜索记录；
- 再使用二分搜索出 2 号和 4 槽的中间位是 (2+4)/2= 3，3 号槽里最大的记录为 12。因为 11 < 12，所以主键为 11 的记录在 3 号槽里；

> 这里有个问题，**「槽对应的值都是这个组的主键最大的记录，如何找到组里最小的记录」**？比如槽 3 对应最大主键是 12 的记录，那如何找到最小记录 9。
>
> 解决办法是：通过槽 3 找到 槽 2 对应的记录，也就是主键为 8 的记录。主键为 8 的记录的下一条记录就是槽 3 当中主键最小的 9 记录，然后开始向下搜索 2 次，定位到主键为 11 的记录，取出该条记录的信息即为我们想要查找的内容。

因为槽内记录有限制，所以即便是复杂度为O（N）的槽内查询，也不会导致复杂度过高。

槽内的记录就只有几条：

- 第一个分组中的记录只能有 1 条记录；
- 最后一个分组中的记录条数范围只能在 1-8 条之间；
- 剩下的分组中记录条数范围只能在 4-8 条之间。

### 2、B+树是如何查询的

#### 2.1、B+树的特点

InnoDB 里的 B+ 树中的**每个节点都是一个数据页**，B+树有以下特点：

- 只**有叶子节点（最底层的节点）才存放了数据**，非叶子节点（其他上层节）仅用来存放目录项作为索引。
- 非叶子节点分为不同层次，通过**分层来降低每一层的搜索量**；
- 所有节点按照索引键大小排序，构成一个**双向链表**，便于范围查询；

#### 2.2、B+树查找记录方式

如何实现快速查找主键为 6 的记录：

![图片](https://cdn.xiaolincoding.com//mysql/other/7c635d682bd3cdc421bb9eea33a5a413.png)

- **定位页**：从根节点开始，通过**二分法快速定位到符合页内范围包含查询值的页**，因为查询的主键值为 6，在[1, 7)范围之间，所以到页 30 中查找更详细的目录项；

  在非叶子节点（页30）中，继续通过二分法快速定位到符合页内范围包含查询值的页，主键值大于 5，所以就到叶子节点（页16）查找记录；

- **定位记录**：接着，在叶子节点（页16）中，通过**槽**查找记录时，使用二分法快速定位要查询的记录在哪个槽（哪个记录分组），定位到槽后，再遍历槽内的所有记录，找到主键为 6 的记录。

可以看到，在定位记录所在哪一个页时，也是通过二分法快速定位到包含该记录的页。**定位到该页后，又会在该页内进行二分法快速定位记录所在的分组（槽号）**，最后在分组内进行遍历查找。

### 3、聚簇索引和二级索引

索引又可以分成聚簇索引和非聚簇索引（二级索引），它们区别就在于叶子节点存放的是什么数据：

- 聚簇索引的叶子节点存放的是实际数据，所有完整的用户记录都存放在聚簇索引的叶子节点；**一个表只能有一个聚簇索引。**
- 二级索引的叶子节点存放的是主键值，而不是实际数据。**一个表可以有多个二级索引。**

#### 3.1、聚簇索引创建

InnoDB 在创建聚簇索引时，会根据不同的场景选择不同的列作为索引：

- 如果有主键，默认会使用主键作为聚簇索引的索引键；
- 如果没有主键，就选择第一个不包含 NULL 值的唯一列作为聚簇索引的索引键；
- 在上面两个都没有的情况下，InnoDB 将自动生成一个隐式自增 id 列作为聚簇索引的索引键；

#### 3.2、聚簇索引和和二级索引的不同

一张表只能有一个聚簇索引，那为了实现非主键字段的快速搜索，就引出了二级索引（非聚簇索引/辅助索引），它也是利用了 B+ 树的数据结构，但是**二级索引的叶子节点存放的是主键值，不是实际数据。**

如果某个查询语句使用了二级索引，但是**查询的数据不是主键值，这时在二级索引找到主键值后，需要去聚簇索引中获得数据行，这个过程就叫作「回表」，也就是说要查两个 B+ 树才能查到数据**。

不过，当查询的数据是主键值、或者是查询的列在二级索引中时，因为只在二级索引就能查询到，不用再去聚簇索引查，这个过程就叫作「**索引覆盖**」，也就是只需要查一个 B+ 树就能找到数据。

### 4、总结

#### 数据页-双向链表、记录-单向链表

InnoDB 的数据是按「**数据页**」为单位来读写的，默认数据页大小为 16 KB。每个**数据页之间通过双向链表**的形式组织起来，物理上不连续，但是逻辑上连续。

数据页内包含用户记录，每个记录之间用**单向链表**的方式组织起来，为了加快在数据页内高效查询记录，设计了一个**页目录**，页目录存储各个槽（分组），且主键值是有序的，于是可以通过**二分查找法**的方式进行检索从而提高效率。

为了高效查询记录所在的数据页，InnoDB 采用 **b+ 树**作为索引，每个节点都是一个数据页。

如果**叶子节点存储的是实际数据的就是聚簇索引**，一个表只能有一个聚簇索引；如果叶子节点存储的不是实际数据，而是**主键值则就是二级索引**，一个表中可以有多个二级索引。

在使用二级索引进行查找数据时，如果查询的数据能在二级索引找到，那么就是「索引覆盖」操作，如果查询的数据不在二级索引里，就需要先在二级索引找到主键值，需要去聚簇索引中获得数据行，这个过程就叫作「回表」。

### 5、MySQL使用B+树的原因

由于磁盘读写速度远远慢于内存，要设计一个适合 MySQL 索引的数据结构，至少满足以下要求：

- 能在**尽可能少的磁盘的 I/O 操作**中完成查询工作；
- 要能高效地查询某一个记录，也要能高效地执行**范围查找**；

#### 5.1、二分查找

在有序数组中可以二分查找高效定位数据。

#### 5.2、二分查找树

用**数组**来实现二分查找有一个问题：**在插入新元素时效率太低**。因为插入元素时，需要将这之后的元素后移一位。

**二叉查找树的特点是一个节点的左子树的所有节点都小于这个节点，右子树的所有节点都大于这个节点**，这样我们在查询数据时，不需要计算中间节点的位置了，只需将查找的数据与节点的数据进行比较。

但是二分查找树有严重的缺点：**当每次插入的元素都是二叉查找树中最大的元素，二叉查找树就会退化成了一条链表，查找数据的时间复杂度变成了 O(n)。**

#### 5.3、自平衡二叉树

为了解决二叉查找树会在极端情况下退化成链表的问题，后面就有人提出**平衡二叉查找树（AVL 树）**。

主要是在二叉查找树的基础上增加了一些条件约束：**每个节点的左子树和右子树的高度差不能超过 1**。也就是说节点的左子树和右子树仍然为平衡二叉树，这样查询操作的时间复杂度就会一直维持在 O(logn) 。

#### 5.4、B树

自平衡二叉树虽然能保持查询操作的时间复杂度在O(logn)，但是因为它本质上是一个二叉树，每个节点只能有 2 个子节点，那么**当节点个数越多的时候，树的高度也会相应变高，这样就会增加磁盘的 I/O 次数，从而影响数据查询的效率。**

**B 树的每一个节点最多可以包括 M 个子节点，M 称为 B 树的阶，所以 B 树就是一个多叉树。**

> 举例：M = 3，那么就是一棵 3 阶的 B 树，特点就是每个节点最多有 2 个（M-1个）数据和最多有 3 个（M个）子节点。

#### 5.5、B+树

B+ 树与 B 树差异的点，主要是以下这几点：

- 叶子节点（**最底部的节点**）才会存放实际数据（**索引+记录**），**非叶子节点只会存放索引**；
- 所有索引都会在叶子节点出现，**叶子节点之间构成一个有序链表**；
- **非叶子节点的索引也会同时存在在子节点中，并且是在子节点中所有索引的最大（或最小）**。
- 非叶子节点中**有多少个子节点，就有多少个索引**；

##### 5.5.1、单点查询

B 树进行单个索引查询时，最快可以在 O(1) 的时间代价内就查到，而从平均时间代价来看，会比 B+ 树稍快一些。

但是 B 树的**查询波动会比较大**，因为每个节点即存索引又存记录，所以有时候访问到了非叶子节点就可以找到索引，而有时需要访问到叶子节点才能找到索引。

B+ 树的非叶子节点不存放实际的记录数据，**仅存放索引**，因此数据量相同的情况下，相比既存索引又存记录的 B 树，**B+树的非叶子节点可以存放更多的索引**，因此 B+ 树可以比 B 树**更「矮胖」**，查询底层节点的磁盘 I/O次数会更少。

##### 5.5.2、插入和删除效率

B+ 树有大量的冗余节点，这样使得删除一个节点的时候，可以直接从叶子节点中删除，甚至可以不动非叶子节点，这样删除非常快。

但是B树删除节点时可能会导致非常复杂的树形变化。

因此，**B+ 树的插入和删除效率更高。**

##### 5.5.3、范围查询

B 树和 B+ 树等值查询原理基本一致，先从根节点查找，然后对比目标数据的范围，最后递归的进入子节点查找。

因为 **B+ 树所有叶子节点间还有一个链表进行连接，这种设计对范围查找非常有帮助。**而 B 树没有将所有叶子节点用链表串联起来的结构，因此只能通过树的遍历来完成范围查询，这会涉及多个节点的磁盘 I/O 操作，范围查询效率不如 B+ 树。

因此，存在大量范围检索的场景，适合使用 B+树，比如数据库。而对于大量的单个索引查询的场景，可以考虑 B 树，比如 nosql 的MongoDB。

#### 5.6、MySQL中的B+树

Innodb 使用的 B+ 树有一些特别的点，比如：

- B+ 树的叶子节点之间是用「双向链表」进行连接，这样的好处是既能向右遍历，也能向左遍历。
- B+ 树点节点内容是数据页，数据页里存放了用户的记录以及各种信息，每个数据页默认大小是 16 KB。

Innodb 根据**索引类型不同，分为聚集和二级索引**。 InnoDB 存储引擎一定会为表创建一个聚集索引，且由于数据在物理上只会保存一份，所以聚簇索引只能有一个，而二级索引可以创建多个。

#### 5.7总结

InnoDB 是 MySQL 默认的存储引擎，它就是采用了 B+ 树作为索引的数据结构。

要设计一个 MySQL 的索引数据结构，不仅仅考虑数据结构**增删改的时间复杂度**，更重要的是要考虑**磁盘 I/0 的操作次数**。因为索引和记录都是存放在硬盘，硬盘是一个非常慢的存储设备，我们在查询数据的时候，最好能在尽可能少的磁盘 I/0 的操作次数内完成。

**二分查找树**虽然是一个天然的二分结构，能很好的利用二分查找快速定位数据，但是它存在一种极端的情况，每当插入的元素都是树内最大的元素，就会导致二分查找树**退化成一个链表**，此时查询复杂度就会从 O(logn)降低为 O(n)。

为了解决二分查找树退化成链表的问题，就出现了**自平衡二叉树**，保证了查询操作的时间复杂度就会一直维持在 O(logn) 。但是它本质上还是一个二叉树，每个节点只能有 2 个子节点，随着元素的增多，树的高度会越来越高。而树的高度决定于磁盘 I/O 操作的次数，就会影响查询性能。

B 树和 B+ 都是通过多叉树的方式，会将树的高度变矮，所以这两个数据结构非常适合检索存于磁盘中的数据。

MySQL 默认的存储引擎 InnoDB 采用的是 B+ 作为索引的数据结构，原因有：

- B+ 树的非叶子节点不存放实际的记录数据，仅存放索引，因此数据量相同的情况下，相比存储即存索引又存记录的 B 树，**B+树的非叶子节点可以存放更多的索引**，因此 B+ 树可以比 B 树更「矮胖」，查询底层节点的磁盘 I/O次数会更少。
- B+ 树有大量的冗余节点（所有非叶子节点都是冗余索引），这些冗余索引让 **B+ 树在插入、删除的效率都更高**，比如删除根节点的时候，不会像 B 树那样会发生复杂的树的变化；
- B+ 树叶子节点之间用链表连接了起来，**有利于范围查询**，而 B 树要实现范围查询，因此只能通过树的遍历来完成范围查询，这会涉及多个节点的磁盘 I/O 操作，范围查询效率不如 B+ 树。



## 十三、索引性能

### 1、MySQL单表不要超过2000W行

- MySQL 的表数据是以页的形式存放的，页在磁盘中不一定是连续的。
- 页的空间是 16K, 并不是所有的空间都是用来存放数据的，会有一些固定的信息，如，页头，页尾，页码，校验码等等。
- 在 B+ 树中，叶子节点和非叶子节点的数据结构是一样的，区别在于，叶子节点存放的是实际的行数据，而非叶子节点存放的是主键和页号。
- 索引结构不会影响单表最大行数，2000W 也只是推荐值，**超过了这个值可能会导致 B + 树层级更高，影响查询性能。**

### 2、索引失效

#### 2.1、对索引使用左模糊、左右模糊匹配

也就是 `like %xx` 或者 `like %xx%` 这两种方式都会造成索引失效。

左模糊是指%林，左边的信息不确定。

#### 2.2、对索引字段使用函数

例如在对索引字段使用len()函数后，就会失效，因**为索引保存的是原始的字段值，而不是函数计算后的值。**如果还想使用索引，就对函数计算后的值也建立索引。

#### 2.3、对索引进行表达式计算

一旦对索引字段进行了表达式计算，例如+1，-1就会索引失效。

#### 2.4、对索引进行隐式转换

**MySQL 在遇到字符串和数字比较的时候，会自动把字符串转为数字，然后再进行比较**。

> 例子一：select * from t_user where phone = 1300000001;
>
> 这里的phone是varchar类型。因为 phone 字段为字符串，会自动把字符串转为数字，所以这条语句相当于：
>
> ```sql
> select * from t_user where CAST(phone AS signed int) = 1300000001;
> ```
>
> 这相当于是对phone字段使用了CAST函数，这就是**对索引使用函数**，导致索引失效。
>
> 例子二：select * from t_user where id = "1";
>
> 这里的id是int类型。因为字符串部分是输入参数，也就需要将字符串转为数字，所以这条语句相当于：
>
> ```sql
> select * from t_user where id = CAST("1" AS signed int);
> ```
>
> 索引字段并没有用任何函数，**CAST 函数是用在了输入参数**，因此是**可以走索引扫描的**。

#### 2.5、联合索引非最左匹配

对主键字段建立的索引叫做聚簇索引，对普通字段建立的索引叫做二级索引。

那么**多个普通字段组合在一起创建的索引就叫做联合索引**，也叫组合索引。联合索引要能正确使用需要遵循**最左匹配原则**，也就是按照最左优先的方式进行索引的匹配。

> 如果查询条件是以下这几种，因为不符合最左匹配原则，所以就无法匹配上联合索引，联合索引就会失效:
>
> - where b=2；
> - where c=3；
> - where b=2 and c=3；

有一个比较特殊的查询条件：where a = 1 and c = 3 ，符合最左匹配吗？这种其实严格意义上来说是属于**索引截断**，不同版本处理方式也不一样。

MySQL 5.5 的话，前面 a 会走索引，在联合索引找到主键值后，开始**回表**，到主键索引读取数据行，Server 层从存储引擎层获取到数据行后，然后在 Server 层再比对 c 字段的值。

从 MySQL 5.6 之后，有一个**索引下推功能**，可以在存储引擎层进行索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，再返还给 Server 层，从而减少回表次数。

**索引下推的大概原理是**：截断的字段不会在 Server 层进行条件判断，而是会被**下推到「存储引擎层」进行条件判断**（因为 c 字段的值是在 `(a, b, c)` 联合索引里的），然后**过滤出符合条件的数据后再返回给 Server 层**。由于在引擎层就过滤掉大量的数据，无需再回表读取数据来进行判断，减少回表次数，从而提升了性能。

#### 2.6、where子句中的or

在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。

因为or条件会对两个列都进行判断，**只要一个不是索引列，就会全表扫描。**

#### 2.7、总结

- 当我们使用左或者左右模糊匹配的时候，也就是 `like %xx` 或者 `like %xx%`这两种方式都会造成索引失效；
- 当我们在查询条件中对索引列使用函数，就会导致索引失效。
- 当我们在查询条件中对索引列进行表达式计算，也是无法走索引的。
- MySQL 在遇到字符串和数字比较的时候，会自动把字符串转为数字，然后再进行比较。如果字符串是索引列，而条件语句中的输入参数是数字的话，那么索引列会发生**隐式类型转换**，由于隐式类型转换是通过 CAST 函数实现的，**等同于对索引列使用了函数**，所以就会导致索引失效。
- 联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。
- 在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。

#### 2.8、模糊匹配索引失效例子

1、select * from s where name like “xxx”；

2、select * from s where name like “xxx%”；

3、select * from s where name like “%xxx”；

4、select * from s where name like “%xxx%”；

##### 2.8.1、情景一

表中有多个字段，id 是自增主键索引，name 是二级索引，其他字段都是非索引字段。

1、2会走索引扫描，3、4会发生索引失效，进行全表扫描。

##### 2.8.2、情景一

表中有2个字段，id 是自增主键索引，name 是二级索引，没有其他字段。

1、2会走索引扫描，3、4用上了**二级索引**，而且用上了**覆盖索引**。

这张表的字段没有「非索引」字段，所以 `select *` 相当于 `select id,name`，然后**这个查询的数据都在二级索引的 B+ 树，因为二级索引的 B+ 树的叶子节点包含「索引值+主键值」，所以查二级索引的 B+ 树就能查到全部结果了，这个就是覆盖索引。**

所以3、4是全扫描二级索引树得到的结果。

> 为什么全扫描二级索引树，而不是扫描聚簇索引树？
>
> 因为**二级索引树的记录东西很少，就只有「索引列+主键值」**，而聚簇索引记录的东西会更多，比如聚簇索引中的叶子节点则记录了**主键值、事务 id、用于事务和 MVCC 的回滚指针以及所有的剩余列**。
>
> 再加上，这个 select * 不用执行回表操作。所以， **MySQL 优化器认为直接遍历二级索引树要比遍历聚簇索引树的成本要小的多**，因此 MySQL 选择了「全扫描二级索引树」的方式查询数据。

为什么表中是多个字段之后，就需要全表扫描，而不是查询聚簇索引树呢？

加了其他字段后，`select * from t_user where name like "%xx";` 要查询的数据就**不能只在二级索引树里找**了，得需要**回表**操作才能完成查询的工作。

这时查询需要在二级索引树中逐一遍历，获取主键后，**再到聚簇索引树中**检索到对应的数据行。优化器认为上面这样的查询过程的成本实在太高了，所以直接选择**全表扫描**的方式来查询数据。

##### 2.8.3、左模糊匹配%xx一定全表扫描吗？

使用左模糊匹配（like "%xx"）并不一定会走全表扫描，**关键还是看数据表中的字段。**

### 3、count(*) 和 count(1) 

![图片](https://cdn.xiaolincoding.com//mysql/other/af711033aa3423330d3a4bc6baeb9532.png)

#### 3.1、count() 是什么？

count() 是一个聚合函数，函数的参数不仅可以是字段名，也可以是其他任意表达式，该函数作用是**统计符合查询条件的记录中，函数指定的参数不为 NULL 的记录有多少个**。

#### 3.2、count(主键字段)

情况一：如果表里**只有主键索引，没有二级索引**时，那么，InnoDB **循环遍历聚簇索引**，将读取到的记录返回给 server 层，然后读取记录中的 id 值，就会 id 值判断是否为 NULL，如果不为 NULL，就将 count 变量加 1。

情况二：如果表里有二级索引时，InnoDB 循环遍历的对象就不是聚簇索引，而是二级索引。因为**相同数量的二级索引记录可以比聚簇索引记录占用更少的存储空间，所以二级索引树比聚簇索引树小**，这样遍历二级索引的 I/O 成本比遍历聚簇索引的 I/O 成本小，因此「优化器」优先选择的是二级索引。

#### 3.3、count(1)

读取主键索引还是二级索引，和count(主键字段)是一样的。

将读取到的记录返回给 server 层，**但是不会读取记录中的任何字段的值**，因为 count 函数的参数是 1，不是字段，所以不需要读取记录中的字段值。

因此，**count(1)的效率比count(主键字段)高**。

#### 3.4、count(*)

*在select和count中的语义不同。

**count(\*) 执行过程跟 count(1) 执行过程基本一样的**，性能没有什么差异。

而且 MySQL 会对 count(*) 和 count(1) 有个优化，如果有多个二级索引的时候，优化器会使用**key_len 最小的二级索引进行扫描**。只有当没有二级索引的时候，才会采用主键索引来进行统计。

#### 3.5、count(字段)

对于这个查询来说，会采用**全表扫描**的方式来计数，所以它的执行效率是比较差的。

#### 3.6小结

count(1)、 count(*)、 count(主键字段)在执行的时候，如果表里存在二级索引，**优化器就会选择二级索引进行扫描**。

所以，如果要执行 count(1)、 count(*)、 count(主键字段) 时，**尽量在数据表上建立二级索引**，这样优化器会自动采用**字段长度最小**的二级索引进行扫描，相比于扫描主键索引效率会高一些。

再来，就是不要使用 count(字段) 来统计记录个数，因为它的效率是最差的，会采用全表扫描的方式来统计。如果你非要统计表中该字段不为 NULL 的记录个数，建议给这个字段建立一个二级索引。

### 4、优化count(*)

#### 4.1、近似值

如果对结果精度要求不高，可以使用 show table status 或者 explain 命令来表进行估算。

InnoDB中的 show table status 和 explain是会返回估计的行数。

执行 explain 命令效率是很高的，因为它**并不会真正的去查询。**

#### 4.2、额外表保存计数值

想精确的获取表的记录总数，我们可以将这个计数值保存到单独的一张计数表中。

当我们在数据表插入一条记录的同时，将计数表中的计数字段 + 1。也就是说，在新增和删除操作时，我们需要额外维护这个计数表。



## 十四、事务特性ACID

并不是所有的引擎都能支持事务，比如 MySQL 原生的 MyISAM 引擎就不支持事务，也正是这样，所以大多数 MySQL 的引擎都是用 InnoDB。

### 1、原子性**（Atomicity）**

一个事务中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节，而且事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。

### 2、**一致性（Consistency）**

是指事务操作前和操作后，**数据满足完整性约束，数据库保持一致性状态**。不会出现用户 A 扣除了 200 元，但用户 B 未增加的情况。

### 3、**隔离性（Isolation）**

数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性**可以防止多个事务并发执行时由于交叉执行而导致数据的不一致**，因为多个事务同时使用相同的数据时，不会相互干扰，每个事务都有一个完整的数据空间，对其他并发事务是隔离的。也就是说，消费者购买商品这个事务，是不影响其他消费者购买的。

### 4、**持久性（Durability）**

事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

### 5、ACID的实现手段

- 原子性是通过 undo log（回滚日志） 来保证的；
- 隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的；
- 持久性是通过 redo log （重做日志）来保证的；
- 一致性则是通过持久性+原子性+隔离性来保证；



## 十五、并行事务

MySQL 服务端是允许多个客户端连接的，这意味着 MySQL 会出现同时处理多个事务的情况。

那么**在同时处理多个事务的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题**。

### 1、脏读-读到未提交事务数据

**如果一个事务「读到」了另一个「未提交事务修改过的数据」，就意味着发生了「脏读」现象。**

事务 A 发生了回滚，那么事务 B 刚才得到的数据就是过期的数据，这种现象就被称为脏读。

### 2、不可重复读-同一事务两次读，数据不同

**在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了「不可重复读」现象。**

### 3、幻读同一事务两次读，数量不同

**在一个事务内多次查询某个符合查询条件的「记录数量」，如果出现前后两次查询到的记录数量不一样的情况，就意味着发生了「幻读」现象。**



## 十六、事务的隔离级别

### 1、并行事物的三种问题

- 脏读：读到其他事务未提交的数据；
- 不可重复读：前后读取的数据不一致；
- 幻读：前后读取的记录数量不一致。

这三个现象的严重性排序如下：

![图片](https://cdn.xiaolincoding.com//mysql/other/d37bfa1678eb71ae7e33dc8f211d1ec1.png)

### 2、四种隔离级别

SQL 标准提出了四种隔离级别来规避这些现象，隔离级别越高，性能效率就越低，这四个隔离级别如下：

- **读未提交（read uncommitted）**，指一个事务还没提交时，它做的变更就能被其他事务看到；
- **读提交（read committed）**，指一个事务提交之后，它做的变更才能被其他事务看到；
- **可重复读（repeatable read）**，指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，**MySQL InnoDB 引擎的默认隔离级别**；
- **串行化（serializable）**；会对记录加上读写锁，在多个事务对这条记录进行读写操作时，如果发生了读写冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行；

按隔离水平高低排序如下：

![图片](https://cdn.xiaolincoding.com//mysql/other/cce766a69dea725cd8f19b90db2d0430.png)

#### 2.1、四种隔离解决的问题

- 在「读未提交」隔离级别下，可能发生脏读、不可重复读和幻读现象；
- 在「读提交」隔离级别下，可能发生不可重复读和幻读现象，但是不可能发生脏读现象；
- 在「可重复读」隔离级别下，可能发生幻读现象，但是不可能脏读和不可重复读现象；
- 在「串行化」隔离级别下，脏读、不可重复读和幻读现象都不可能会发生。

要解决脏读现象，就要升级到「读提交」以上的隔离级别；

要解决不可重复读现象，就要升级到「可重复读」的隔离级别；

要解决幻读现象，将隔离级别升级到「串行化」，但是不建议使用。

![图片](https://cdn.xiaolincoding.com//mysql/other/4e98ea2e60923b969790898565b4d643.png)

MySQL 在**「可重复读」隔离级别下，可以很大程度上避免幻读现象的发生**（注意是很大程度避免，并不是彻底避免），所以 MySQL 并不会使用「串行化」隔离级别来避免幻读现象的发生，因为使用「串行化」隔离级别会影响性能。

#### 2.2、四种隔离级别实现

![图片](https://cdn.xiaolincoding.com//mysql/other/d5de450e901ed926d0b5278c8b65b9fe.png)

在不同隔离级别下，事务 A 执行过程中查询到的余额可能会不同：

- 在「读未提交」隔离级别下，事务 B 修改余额后，虽然没有提交事务，但是此时的余额已经可以被事务 A 看见了，于是事务 A 中余额 V1 查询的值是 200 万，余额 V2、V3 自然也是 200 万了；
- 在「读提交」隔离级别下，事务 B 修改余额后，因为没有提交事务，所以事务 A 中余额 V1 的值还是 100 万，等事务 B 提交完后，最新的余额数据才能被事务 A 看见，因此额 V2、V3 都是 200 万；
- 在「可重复读」隔离级别下，事务 A **只能看见启动事务时的数据**，所以余额 V1、余额 V2 的值都是 100 万，当事务 A 提交事务后，就能看见最新的余额数据了，所以余额 V3 的值是 200 万；
- 在「串行化」隔离级别下，事务 B 在执行将余额 100 万修改为 200 万时，**由于此前事务 A 执行了读操作，这样就发生了读写冲突，于是就会被锁住**，直到事务 A 提交后，事务 B 才可以继续执行，所以从 A 的角度看，余额 V1、V2 的值是 100 万，余额 V3 的值是 200万。

这四种隔离级别具体是如何实现的呢？

- 对于「读未提交」隔离级别的事务来说，因为可以读到未提交事务修改的数据，所以**直接读取最新的数据**就好了；

- 对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 **Read View** 来实现的，它们的区别在于创建 Read View 的时机不同。**Read View**类似于一个数据快照。

  「**读提交**」隔离级别是在「**每个语句执行前**」都会重新生成一个 Read View，而「**可重复读**」隔离级别是「**启动事务时**」生成一个 Read View，然后整个事务期间都在用这个 Read View。

- 对于「串行化」隔离级别的事务来说，通过**加读写锁**的方式来避免并行访问；

### 3、Read View在MVCC**（多版本并发控制）**中的工作

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/readview%E7%BB%93%E6%9E%84.drawio.png)

Read View 有四个重要的字段：

- m_ids ：指的是在创建 Read View 时，当前数据库中「活跃事务」的**事务 id 列表**，**“活跃事务”指的就是，启动了但还没提交的事务**。
- min_trx_id ：指的是在创建 Read View 时，当前数据库中「活跃事务」中事务 **id 最小的事务**，也就是 m_ids 的最小值。
- max_trx_id ：这个并不是 m_ids 的最大值，而是**创建 Read View 时当前数据库中应该给下一个事务的 id 值**，也就是全局事务中最大的事务 id 值 + 1；
- creator_trx_id ：指的是**创建该 Read View 的事务的事务 id**。

对于使用 InnoDB 存储引擎的数据库表，它的聚簇索引记录中都包含下面两个隐藏列：

- trx_id，当一个事务对某条聚簇索引记录进行改动时，就会**把该事务的事务 id 记录在 trx_id 隐藏列里**；
- roll_pointer，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后**这个隐藏列是个指针，指向每一个旧版本记录**，于是就可以通过它找到修改前的记录。

在创建 Read View 后，我们可以将记录中的 trx_id 划分这三种情况：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/ReadView.drawio.png)

一个事务去访问记录的时候，除了自己的更新记录总是可见之外，还有这几种情况：

- 如果记录的 trx_id 值小于 Read View 中的 `min_trx_id` 值，表示这个版本的记录是在创建 Read View **前**已经提交的事务生成的，所以该版本的记录对当前事务**可见**。
- 如果记录的 trx_id 值大于等于 Read View 中的 `max_trx_id` 值，表示这个版本的记录是在创建 Read View **后**才启动的事务生成的，所以该版本的记录对当前事务**不可见**。
- 如果记录的 trx_id 值在 Read View 的 min_trx_id和max_trx_id 之间，需要判断 trx_id 是否在 m_ids 列表中：
  - 如果记录的 trx_id **在** `m_ids` 列表中，表示生成该版本记录的活跃事务依然活跃着（还没提交事务），所以该版本的记录对当前事务**不可见**。
  - 如果记录的 trx_id **不在** `m_ids`列表中，表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务**可见**。

**这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）**

### 4、可重复读的工作机制（解决脏读、不可重复读）

**可重复读隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View**。

假设事务 A （事务 id 为51）启动后，紧接着事务 B （事务 id 为52）也启动了，那这两个事务创建的 Read View 如下：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/%E4%BA%8B%E5%8A%A1ab%E7%9A%84%E8%A7%86%E5%9B%BE-new.png)

事务 A 和 事务 B 的 Read View 具体内容如下：

- 在事务 A 的 Read View 中，它的事务 id 是 51，由于它是第一个启动的事务，所以此时活跃事务的事务 id 列表就只有 51，活跃事务的事务 id 列表中最小的事务 id 是事务 A 本身，下一个事务 id 则是 52。
- 在事务 B 的 Read View 中，它的事务 id 是 52，由于事务 A 是活跃的，所以此时活跃事务的事务 id 列表是 51 和 52，**活跃的事务 id 中最小的事务 id 是事务 A**，下一个事务 id 应该是 53。

接着，在可重复读隔离级别下，事务 A 和事务 B 按顺序执行了以下操作：

- 事务 B 读取小林的账户余额记录，读到余额是 100 万；
- 事务 A 将小林的账户余额记录修改成 200 万，并没有提交事务；
- 事务 B 读取小林的账户余额记录，读到余额还是 100 万；
- 事务 A 提交事务；
- 事务 B 读取小林的账户余额记录，读到余额依然还是 100 万；

事务 B 第一次读小林的账户余额记录，在找到记录后，它会先看这条记录的 trx_id，此时**发现 trx_id 为 50，比事务 B 的 Read View 中的 min_trx_id 值（51）还小，这意味着修改这条记录的事务早就在事务 B 启动前提交过了，所以该版本的记录对事务 B 可见的**，也就是事务 B 可以获取到这条记录。

接着，事务 A 通过 update 语句将这条记录修改了（还未提交事务），将小林的余额改成 200 万，这时 MySQL 会记录相应的 undo log，并以链表的方式串联起来，形成**版本链**，如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/%E4%BA%8B%E5%8A%A1ab%E7%9A%84%E8%A7%86%E5%9B%BE2.png)

在上图的「记录的字段」看到，由于事务 A 修改了该记录，以前的记录就变成旧版本记录了，于是最新记录和旧版本记录通过链表的方式串起来，而且最新记录的 trx_id 是事务 A 的事务 id（trx_id = 51）。

然后事务 B 第二次去读取该记录，**发现这条记录的 trx_id 值为 51，在事务 B 的 Read View 的 min_trx_id 和 max_trx_id 之间，则需要判断 trx_id 值是否在 m_ids 范围内，判断的结果是在的，那么说明这条记录是被还未提交的事务修改的，这时事务 B 并不会读取这个版本的记录。而是沿着 undo log 链条往下找旧版本的记录，直到找到 trx_id 「小于」事务 B 的 Read View 中的 min_trx_id 值的第一条记录**，所以事务 B 能读取到的是 trx_id 为 50 的记录，也就是小林余额是 100 万的这条记录。

最后，当事物 A 提交事务后，**由于隔离级别时「可重复读」，所以事务 B 再次读取记录时，还是基于启动事务时创建的 Read View 来判断当前版本的记录是否可见。所以，即使事物 A 将小林余额修改为 200 万并提交了事务， 事务 B 第三次读取记录时，读到的记录都是小林余额是 100 万的这条记录**。

### 5、读提交的工作机制（解决脏读、但是不可重复读）

**读提交隔离级别是在每次读取数据时，都会生成一个新的 Read View**。事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。

假设事务 A （事务 id 为51）启动后，紧接着事务 B （事务 id 为52）也启动了，接着按顺序执行了以下操作：

- 事务 B 读取数据（创建 Read View），小林的账户余额为 100 万；
- 事务 A 修改数据（还没提交事务），将小林的账户余额从 100 万修改成了 200 万；
- 事务 B 读取数据（创建 Read View），小林的账户余额为 100 万；
- 事务 A 提交事务；
- 事务 B 读取数据（创建 Read View），小林的账户余额为 200 万；

重点看事务 B 每次读取数据时创建的 Read View。前两次 事务 B 读取数据时创建的 Read View 如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/%E8%AF%BB%E6%8F%90%E4%BA%A4%E4%BA%8B%E5%8A%A1.png)

> 为什么事务 B 第二次读数据时，读不到事务 A （还未提交事务）修改的数据？
>
> 事务 B 在找到小林这条记录时，会看这条记录的 trx_id 是 51，在事务 B 的 Read View 的 min_trx_id 和 max_trx_id 之间，接下来需要判断 trx_id 值是否在 m_ids 范围内，判断的结果是在的，那么说明**这条记录是被还未提交的事务修改的，这时事务 B 并不会读取这个版本的记录**。而是，**沿着 undo log 链条**往下找旧版本的记录，直到找到 trx_id 「小于」事务 B 的 Read View 中的 min_trx_id 值的第一条记录，所以事务 B 能读取到的是 trx_id 为 50 的记录，也就是小林余额是 100 万的这条记录。

> 为什么事务 A 提交后，事务 B 就可以读到事务 A 修改的数据？
>
> 在事务 A 提交后，**由于隔离级别是「读提交」，所以事务 B 在每次读数据的时候，会重新创建 Read View**，此时事务 B 第三次读取数据时创建的 Read View 如下：
>
> ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/%E8%AF%BB%E6%8F%90%E4%BA%A4%E4%BA%8B%E5%8A%A12.drawio.png)
>
> 事务 B 在找到小林这条记录时，**会发现这条记录的 trx_id 是 51，比事务 B 的 Read View 中的 min_trx_id 值（52）还小，这意味着修改这条记录的事务早就在创建 Read View 前提交过了，所以该版本的记录对事务 B 是可见的**。

正是因为在读提交隔离级别下，事务**每次读数据时都重新创建 Read View**，那么在事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。

### 6、总结

事务是在 MySQL 引擎层实现的，我们常见的 InnoDB 引擎是支持事务的，事务的四大特性是**原子性、一致性、隔离性、持久性**，主要讲的是隔离性。

当多个事务并发执行的时候，会引发**脏读、不可重复读、幻读**这些问题，那为了避免这些问题，SQL 提出了四种隔离级别，分别是**读未提交、读已提交、可重复读、串行化**，从左往右隔离级别顺序递增，**隔离级别越高，意味着性能越差**，InnoDB 引擎的**默认隔离级别是可重复读**。

要解决脏读现象，就要将隔离级别升级到读已提交以上的隔离级别，要解决不可重复读现象，就要将隔离级别升级到可重复读以上的隔离级别。

而对于幻读现象，**不建议将隔离级别升级为串行化，因为这会导致数据库并发时性能很差**。MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象

对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 Read View 来实现的，它们的区别在于**创建 Read View 的时机不同**：

- 「**读提交**」隔离级别是在**每个 select 都会生成一个新的 Read View**，也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。
- 「**可重复读**」隔离级别是**启动事务时生成一个 Read View**，然后整个事务期间都在用这个 Read View，这样就保证了在事务期间读到的数据都是事务启动前的记录。

这两个隔离级别实现是通过「事务的 Read View 里的字段」和「记录中的两个隐藏列」的比对，来控制并发事务访问同一个记录时的行为，这就叫 **MVCC（多版本并发控制）。**

### 7、可重复读中的幻读

幻读：当同一个查询在不同的时间产生不同的结果集时，事务中就会出现所谓的幻象问题。例如，如果 SELECT 执行了两次，但**第二次返回了第一次没有返回的行**，则该行是“幻像”行。

#### 7.1、快照读Read View（避免，但是没有完全解决）

可重复读隔离级是由 MVCC（多版本并发控制）实现的，执行第一个查询语句后，会创建一个 Read View，**后续的查询语句利用这个 Read View，通过这个 Read View 就可以在 undo log 版本链找到事务开始时的数据，所以事务过程中每次查询的数据都是一样的**，即使中途有其他事务插入了新纪录，是查询不出来这条数据的，所以就很好了避免幻读问题。

#### 7.2、当前读(增删改当前读，导致了幻读)

MySQL 里除了普通查询是快照读，其他都是**当前读**，比如 update、insert、delete，这些语句执行前都会**查询最新版本的数据**，然后再做进一步的操作。

但是，**增删改**时当前读就会导致**幻读**问题。所以，**Innodb 引擎为了解决「可重复读」隔离级别使用「当前读」而造成的幻读问题，就引出了间隙锁**。

#### 7.3、记录锁+间隙锁 next-key lock

使用记录锁+间隙锁，弥补可重复读中，幻读的部分缺陷。

假设，表中有一个范围 id 为（3，5）间隙锁，那么其他事务就无法插入 id = 4 这条记录了，这样就有效的防止幻读现象的发生。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E9%94%81/gap%E9%94%81.drawio.png)

举个具体例子，场景如下：

![img](https://cdn.xiaolincoding.com//mysql/other/3af285a8e70f4d4198318057eb955520.png)

事务 A 执行了这面这条锁定读语句后，就在对表中的记录加上 id 范围为 (2, +∞] 的 next-key lock（next-key lock 是间隙锁+记录锁的组合）。

然后，事务 B 在执行插入语句的时候，判断到插入的位置被事务 A 加了 next-key lock，于是**事物 B 会生成一个插入意向锁**，同时进入等待状态，**直到事务 A 提交了事务**。这就避免了由于事务 B 插入新记录而导致事务 A 发生幻读的现象。

#### 7.4、快照读+当前读，未能完全解决幻读

**可重复读隔离级别下虽然很大程度上避免了幻读，但是还是没有能完全解决幻读**。

##### 7.4.1、情景一

- T1 时刻：事务 A 执行查询 id = 5 的记录，此时表中是没有该记录的，所以查询不出来。
- T2 时刻：然后事务 B 插入一条 id = 5 的记录，并且提交了事务。
- T3时刻：此时，**事务 A 更新 id = 5 这条记录，对没错，事务 A 看不到 id = 5 这条记录，但是他去更新了这条记录，这场景确实很违和，然后再次查询 id = 5 的记录.**

**事务 A 就能看到事务 B 插入的纪录了，幻读就是发生在这种违和的场景**。

##### 7.4.2、情景二

除了上面这一种场景会发生幻读现象之外，还有下面这个场景也会发生幻读现象。

- T1 时刻：事务 A 先执行「快照读语句」：select * from t_test where id > 100 得到了 3 条记录。
- T2 时刻：事务 B 往插入一个 id= 200 的记录并提交；
- T3 时刻：事务 A 再执行「当前读语句」 select * from t_test where id > 100 for update 就会得到 4 条记录，此时也发生了幻读现象。

**要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select ... for update 这类当前读的语句**，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。

#### 7.5、总结

MySQL InnoDB 引擎的可重复读隔离级别（默认隔离级），根据不同的查询方式，分别提出了避免幻读的方案：

- 针对**快照读**（普通 select 语句），是通过 MVCC 方式解决了幻读。
- 针对**当前读**（select ... for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读。

所以，**MySQL 可重复读隔离级别并没有彻底解决幻读，只是很大程度上避免了幻读现象的发生。**



## 十七、锁基础

### 1、全局锁

#### 1.1、全局锁加锁方式

执行后，**整个数据库就处于只读状态了**，这时其他线程执行以下操作，都会被阻塞：

- 对数据的增删改操作，比如 insert、delete、update等语句；
- 对表结构的更改操作，比如 alter table、drop table 等语句。

此外，当会话断开了，全局锁会被自动释放。

#### 1.2、全局锁应用场景

全局锁主要应用于做**全库逻辑备份**，这样在备份数据库期间，不会因为数据或表结构的更新，而出现备份文件的数据与预期的不一样。

#### 1.3、全局锁缺点

加上全局锁，意味着整个数据库都是只读状态。

那么如果数据库里有很多数据，备份就会花费很多的时间，关键是备份期间，业务只能读数据，而不能更新数据，这样会造成**业务停滞**。

#### 1.4、替代方式

如果数据库的引擎支持的事务支持**可重复读的隔离级别**，那么在备份数据库之前先开启事务，会先创建 Read View，然后整个事务执行期间都在用这个 Read View，而且由于 MVCC 的支持，备份期间业务依然可以对数据进行更新操作。

因为在可重复读的隔离级别下，即使其他事务更新了表的数据，也不会影响备份数据库时的 Read View，这就是事务四大特性中的**隔离性**，这样备份期间备份的数据一直是在开启事务时的数据。

### 2、表级锁

#### 2.1、普通表级锁

需要注意的是，表锁除了会限制别的线程的读写外，**也会限制本线程接下来的读写操作**。也就是说如果本线程对学生表加了「共享表锁」，那么本线程接下来如果要对学生表执行写操作的语句，也是会被阻塞的。

不过尽量避免在使用 InnoDB 引擎的表使用表锁，因为表锁的颗粒度太大，会影响并发性能，**InnoDB 牛逼的地方在于实现了颗粒度更细的行级锁**。

#### 2.2、元数据锁

**元数据锁**（MDL）。

我们不需要显式的使用 MDL，因为当我们对数据库表进行操作时，会自动给这个表加上 MDL：

- 对一张表进行 CRUD 操作时，加的是 **MDL 读锁**；
- 对一张表做结构变更操作的时候，加的是 **MDL 写锁**；

MDL 是为了保证当用户对表执行 CRUD 操作时，防止其他线程对这个表结构做了变更。

##### 2.2.1、MDL读锁

当有线程在执行 select 语句（ 加 MDL 读锁）的期间，如果有其他线程要更改该表的结构（ 申请 MDL 写锁），那么将会被阻塞，直到执行完 select 语句（ 释放 MDL 读锁）。

##### 2.2.2、MDL写锁

当有线程对表结构进行变更（ 加 MDL 写锁）的期间，如果有其他线程执行了 CRUD 操作（ 申请 MDL 读锁），那么就会被阻塞，直到表结构变更完成（ 释放 MDL 写锁）。

##### 2.2.3、MDL何时释放

MDL 是在事务提交后才会释放，这意味着**事务执行期间，MDL 是一直持有的**。这导致一些问题：

在操作队列中，一旦有一个线程申请不到MDL写锁，**后续有对该表的 select 语句，就都会被阻塞**，如果此时有大量该表的 select 语句的请求到来，就会有大量的线程被阻塞住，这时数据库的线程很快就会爆满了。

> 为什么线程 因为申请不到 MDL 写锁，而导致后续的申请读锁的查询操作也会被阻塞？
>
> 这是因为申请 MDL 锁的操作会形成一个队列，队列中**写锁获取优先级高于读锁**，一旦出现 MDL 写锁等待**，会阻塞后续该表的所有 CRUD 操作。**

#### 2.3、意向锁

普通的 select 是不会加行级锁的，**普通的 select 语句是利用 MVCC 实现一致性读，是无锁的**。

##### 2.3.1、意向锁分类

- 在使用 InnoDB 引擎的表里对某些记录加上「共享锁」之前，需要先在表级别加上一个「意向共享锁」；
- 在使用 InnoDB 引擎的表里对某些纪录加上「独占锁」之前，需要先在表级别加上一个「意向独占锁」；

意向共享锁和意向独占锁是**表级锁**，不会和**行级的共享锁和独占锁**发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁和独占表锁发生冲突。

##### 2.3.2、意向锁的作用

如果没有「意向锁」，那么加「独占表锁」时，就需要遍历表里所有记录，查看是否有记录存在独占锁，这样效率会很慢。

那么有了「意向锁」，由于在对记录加独占锁前，先会加上表级别的意向独占锁。那么在加「独占表锁」时，直接查该表是否有意向独占锁，如果有就意味着表里已经有记录被加了独占锁，这样就不用去遍历表里的记录。

所以，**意向锁的目的是为了快速判断表里是否有记录被加锁**。

#### 2.4、AUTO-INC锁

表里的主键通常都会设置成自增的，之后可以在插入数据时，可以不指定主键的值，数据库会自动给主键赋值递增的值，这主要是通过 **AUTO-INC 锁**实现的。

AUTO-INC 锁是特殊的表锁机制，锁**不是再一个事务提交后才释放，而是再执行完插入语句后就会立即释放**。

**在插入数据时，会加一个表级别的 AUTO-INC 锁**，然后为被 `AUTO_INCREMENT` 修饰的字段赋值递增的值，等插入语句执行完成后，才会把 AUTO-INC 锁释放掉。

那么，一个事务在持有 AUTO-INC 锁的过程中，其他事务的如果要向该表插入语句都会被阻塞，**从而保证插入数据时，被 `AUTO_INCREMENT` 修饰的字段的值是连续递增的。**

##### 2.4.1、改进AUTO-INC锁

在 MySQL 5.1.22 版本开始，InnoDB 存储引擎提供了一种**轻量级的锁**来实现自增。

一样也是在插入数据的时候，会为被 `AUTO_INCREMENT` 修饰的字段加上轻量级锁，**然后给该字段赋值一个自增的值，就把这个轻量级锁释放了，而不需要等待整个插入语句执行完后才释放锁**。

### 3、行级锁

InnoDB 引擎是支持行级锁的，而 MyISAM 引擎并不支持行级锁。

共享锁（S锁）满足读读共享，读写互斥。独占锁（X锁）满足写写互斥、读写互斥。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E9%94%81/x%E9%94%81%E5%92%8Cs%E9%94%81.png)

#### 3.1、Record Lock

Record Lock 称为**记录锁**，锁住的是一条记录。而且**记录锁是有 S 锁和 X 锁之分**的：

- 当一个事务对一条记录加了 S 型记录锁后，其他事务也可以继续对该记录加 S 型记录锁（**S 型与 S 锁兼容**），但是不可以对该记录加 X 型记录锁（**S 型与 X 锁不兼容**）;
- 当一个事务对一条记录加了 X 型记录锁后，其他事务既不可以对该记录加 S 型记录锁（**S 型与 X 锁不兼容**），也不可以对该记录加 X 型记录锁（**X 型与 X 锁不兼容**）。

#### 3.2、Gap Lock

Gap Lock 称为**间隙锁**，只存在于**可重复读**隔离级别，目的是为了解决可重复读隔离级别下**幻读**的现象。

假设，表中有一个范围 id 为（3，5）间隙锁，那么其他事务就无法插入 id = 4 这条记录了，这样就有效的防止幻读现象的发生。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E9%94%81/gap%E9%94%81.drawio.png)

间隙锁虽然存在 X 型间隙锁和 S 型间隙锁，但是并没有什么区别，**间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，因为间隙锁的目的是防止插入幻影记录而提出的**。

#### 3.3、 Next-Key Lock

Next-Key Lock 称为**临键锁**，是 Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。

假设，表中有一个范围 id 为（3，5] 的 next-key lock，那么其他事务即不能插入 id = 4 记录，也不能修改 id = 5 这条记录。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E9%94%81/%E4%B8%B4%E9%94%AE%E9%94%81.drawio.png)

所以，next-key lock **即能保护该记录，又能阻止其他事务将新纪录插入到被保护记录前面的间隙中**。

具有相同范围的Next-Key Lock之间会互相阻塞。

#### 3.4、插入意向锁

一个事务在插入一条记录的时候，需要判断插入位置是否已被其他事务加了间隙锁（next-key lock 也包含间隙锁）。

如果有的话，插入操作就会发生**阻塞**，直到拥有间隙锁的那个事务提交为止（释放间隙锁的时刻），在此期间会生成一个**插入意向锁**，表明有事务想在某个区间插入新记录，但是现在处于等待状态。

> 举个例子，假设事务 A 已经对表加了一个范围 id 为（3，5）间隙锁。
>
> ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E9%94%81/gap%E9%94%81.drawio.png)
>
> 当事务 A 还没提交的时候，事务 B 向该表插入一条 id = 4 的新记录，这时会判断到插入的位置已经被事务 A 加了间隙锁，于是事物 B 会生成一个**插入意向锁**，然后将锁的状态设置为等待状态
>
> （*PS：MySQL 加锁时，是先生成锁结构，然后设置锁的状态，如果锁状态是等待状态，并不是意味着事务成功获取到了锁，只有当锁状态为正常状态时，才代表事务成功获取到了锁*）
>
> 此时事务 B 就会发生阻塞，直到事务 A 提交了事务。

插入意向锁名字虽然有意向锁，但是它并**不是意向锁，它是一种特殊的间隙锁，属于行级别锁**。



## 十八、加锁

### 1、什么SQL语句加行级锁？

**普通的 select 语句是不会对记录加锁**的（除了串行化隔离级别），因为它属于快照读，是通过 **MVCC（多版本并发控制）**实现的。

如果要在查询时对记录加行级锁，可以使用下面这两个方式，这两种查询会加锁的语句称为**锁定读**。

```sql
//对读取的记录加共享锁(S型锁)
select ... lock in share mode;

//对读取的记录加独占锁(X型锁)
select ... for update;
```

上面这两条语句必须在一个事务中，**因为当事务提交了，锁就会被释放**，所以在使用这两条语句的时候，要加上 begin 或者 start transaction 开启事务的语句。

**除了上面这两条锁定读语句会加行级锁之外，update 和 delete 操作都会加行级锁，且锁的类型都是独占锁(X型锁)**。

### 2、行级锁的种类

不同隔离级别下，行级锁的种类是不同的。

在读已提交隔离级别下，行级锁的种类只有记录锁，也就是仅仅把一条记录锁上。

在可重复读隔离级别下，行级锁的种类除了有记录锁，还有间隙锁（目的是为了避免幻读），所以行级锁的种类主要有三类：

- Record Lock，**记录锁**，也就是仅仅把一条记录锁上；
- Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身；
- Next-Key Lock：**临键锁**，Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。

### 3、怎么加行级锁

**加锁的对象是索引，加锁的基本单位是 next-key lock**，它是由记录锁和间隙锁组合而成的，**next-key lock 是前开后闭区间，而间隙锁是前开后开区间**。

退化：**在能使用记录锁或者间隙锁就能避免幻读现象的场景下， next-key lock 就会退化成记录锁或间隙锁**。

#### 3.1、唯一索引等值查询

当我们用唯一索引进行等值查询的时候，查询的记录存不存在，加锁的规则也会不同，以查询条件是查询唯一索引id=1为例：

- 当查询的记录是「存在」的，在索引树上定位到这一条记录后，将该记录的索引中的 next-key lock 会**退化成「记录锁」**。

  因为这是主键索引，其他事务插入id=1时会发生**主键冲突**，无法插入。

  因为给id=1加了**记录锁**，所以无法删除。只靠记录锁就能避免幻读。

- 当查询的记录是「不存在」的，在索引树找到第一条大于该查询记录的记录后，将该记录的索引中的 next-key lock 会**退化成「间隙锁」**。

  因为对应的记录id=1不存在，不能加记录锁。

  SQL就根据id=1前后的记录，以前后数据为区间加上间隙锁。

#### 3.2、唯一索引范围查询

#### 3.3、非唯一索引等值查询

#### 3.4、非唯一索引范围查询

#### 3.5、没有加索引的查询

### 4、update没加索引导致锁全表

当我们执行 update 语句时，实际上是会对记录加独占锁（X 锁）的，如果其他事务对持有独占锁的记录进行修改时是会被阻塞的。

另外，这个锁并不是执行完 update 语句就会释放的，而是会**等事务结束时才会释放**。

#### 4.1、锁全表场景

在 InnoDB 事务中，对记录加锁带基本单位是 next-key 锁，但是会因为一些条件会退化成间隙锁，或者记录锁。加锁的位置准确的说，锁是加在索引上的而非行上。

> 比如，在 update 语句的 where 条件使用了唯一索引，那么 next-key 锁会退化成记录锁，也就是只会给一行记录加锁。
>
> ![在这里插入图片描述](https://cdn.xiaolincoding.com//mysql/other/d2326f98cbb34fc09ca4013703251501.png)

**在 update 语句的 where 条件没有使用索引，就会全表扫描，于是就会对所有记录加上 next-key 锁（记录锁 + 间隙锁），相当于把整个表锁住了**。

![img](https://cdn.xiaolincoding.com//mysql/other/1aa886fe95e7bc791c296e2d342fa435.png)

#### 4.2、如何避免

我们可以将 MySQL 里的 `sql_safe_updates` 参数设置为 1，开启**安全更新**模式。

update 语句必须满足如下条件之一才能执行成功：

- 使用 where，并且 where 条件中必须有索引列；
- 使用 limit；
- 同时使用 where 和 limit，此时 where 条件中可以没有索引列；

delete 语句必须满足以下条件能执行成功：

- 同时使用 where 和 limit，此时 where 条件中可以没有索引列；



## 十九、死锁

### 1、死锁原因

可重复读隔离级别下，是存在**幻读**的问题。

**Innodb 引擎为了解决「可重复读」隔离级别下的幻读问题，就引出了 next-key 锁**，它是记录锁和间隙锁的组合。

- Record Lock，记录锁，锁的是记录本身；
- Gap Lock，间隙锁，锁的就是两个值之间的空隙，以防止其他事务在这个空隙间插入新的数据，从而避免幻读现象。

行锁的释放时机是在事务提交（commit）后，锁就会被释放，并不是一条语句执行完就释放行锁。

#### 1.1、死锁案例

![img](https://cdn.xiaolincoding.com//mysql/other/90c1e01d0345de639e3426cea0390e80-20230309222252447.png)

事务 A 在执行下面这条语句的时候：

```sql
select id from t_order where order_no = 1007 for update;
```

**在二级索引（INDEX_NAME : index_order）上加的是 X 型的 next-key 锁，锁范围是`(1006, +∞]`**。

当事务 B 往事务 A next-key 锁的范围 (1006, +∞] 里插入 id = 1008 的记录就会被锁住：

```sql
Insert into t_order (order_no, create_date) values (1008, now());
```

因为当我们执行以下插入语句时，会在插入间隙上获取插入意向锁，**而插入意向锁与间隙锁是冲突的，所以当其它事务持有该间隙的间隙锁时，需要等待其它事务释放间隙锁之后，才能获取到插入意向锁。而间隙锁与间隙锁之间是兼容的，所以所以两个事务中 `select ... for update` 语句并不会相互影响**。

案例中的事务 A 和事务 B 在执行完后 `select ... for update` 语句后都持有范围为`(1006,+∞]`的next-key 锁，而接下来的插入操作为了获取到插入意向锁，**都在等待对方事务的间隙锁释放**，于是就造成了循环等待，导致死锁。

> 为什么间隙锁与间隙锁之间是兼容的？
>
> **间隙锁的意义只在于阻止区间被插入**，因此是可以共存的。**一个事务获取的间隙锁不会阻止另一个事务获取同一个间隙范围的间隙锁**，共享和排他的间隙锁是没有区别的，他们相互不冲突，且功能相同，即两个事务可以同时持有包含共同间隙的间隙锁。
>
> 注意1：**next-key lock 是包含间隙锁+记录锁的，如果一个事务获取了 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的**。
>
> 注意2：但对于**记录锁**，我们是要考虑 X 型与 S 型关系。
>
> 注意3：对于这种范围为 (1006, +∞] 的 next-key lock，两个事务是可以同时持有的，不会冲突。因为 +∞ 并不是一个真实的记录，自然就不需要考虑 X 型与 S 型关系。

#### 1.2、插入意向锁

**插入意向锁是一种特殊的间隙锁，但不同于间隙锁的是，该锁只用于并发插入操作**。如果说间隙锁锁住的是一个区间，那么「插入意向锁」锁住的就是一个点。

**插入意向锁的生成时机：**

每插入一条新记录，都需要看一下待插入记录的下一条记录上是否已经被加了间隙锁，**如果已加间隙锁，此时会生成一个插入意向锁**，然后锁的状态设置为**等待状态**（PS：MySQL 加锁时，是先生成锁结构，然后设置锁的状态，如果锁状态是等待状态，并不是意味着事务成功获取到了锁，**只有当锁状态为正常状态时，才代表事务成功获取到了锁**），现象就是 Insert 语句会被阻塞。

### 2、Insert 语句是怎么加行级锁的？

Insert 语句在**正常执行时是不会生成锁结构**的，它是靠聚簇索引记录自带的 trx_id 隐藏列来作为**隐式锁**来保护记录的。 

#### 2.1、隐式锁

当事务需要加锁的时，如果这个锁不可能发生冲突，InnoDB会**跳过加锁**环节，这种机制称为**隐式锁**。隐式锁是 InnoDB 实现的一种**延迟加锁机制**，其特点是只有在可能发生冲突时才加锁，从而**减少了锁的数量**，提高了系统整体性能。 

隐式锁就是在 Insert 过程中不加锁，只有在**特殊情况**下，才会将隐式锁转换为显示锁 ：

##### 2.1.1、特殊情况1：记录之间加有间隙锁

每插入一条新记录，都需要看一下待插入记录的下一条记录上是否已经被加了间隙锁，**如果已加间隙锁，此时会生成一个插入意向锁**，然后锁的状态设置为等待状态，现象就是 Insert 语句会被阻塞。

##### 2.1.2、特殊情况2：遇到唯一键冲突

如果在插入新记录时，插入了一个与「**已有的记录的主键或者唯一二级索引列值相同**」的记录，此时插入就会失败，然后对于这条记录加上了 **S 型的锁**。

- 如果**主键索引重复**，插入新记录的事务会给已存在的主键值重复的聚簇索引记录**添加 S 型记录锁**。
- 如果**唯一二级索引重复**，插入新记录的事务都会给已存在的二级索引列值重复的二级索引记录**添加 S 型 next-key 锁**。

> 案例：在隔离级别可重复读的情况下，开启两个事务，前后执行相同的 Insert 语句，此时**事务 B 的 Insert 语句会发生阻塞**。
>
> ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E9%94%81/%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%8A%A0%E9%94%81.drawio.png)步骤1：事务 A 先插入 order_no 为 1006 的记录，可以插入成功，此时对应的唯一二级索引记录被「隐式锁」保护。
>
> 步骤2*事务 A 插入的 order_no 值为 1006 的记录上的「隐式锁」会变「**显式锁**」且锁类型为 **X 型**的记录锁，所以事务 B 向获取 **S 型 next-key 锁**时会遇到锁冲突，事务 B 进入阻塞状态。

##### 2.1.3、唯一键冲突时，添加S型索引的作用

解决了唯一索引的重复问题。

并发多个事务的时候，第一个事务插入的记录，并不会加锁，而是会用**隐式锁保护唯一二级索引的记录**。

但是当第一个事务还未提交的时候，有其他事务插入了与第一个事务相同的记录，第二个事务就会**被阻塞**，**因为此时第一事务插入的记录中的隐式锁会变为显示锁且类型是 X 型的记录锁，而第二个事务是想对该记录加上 S 型的 next-key 锁，X 型与 S 型的锁是冲突的**，所以导致第二个事务会等待，直到第一个事务提交后，释放了锁。

如果 order_no 不是唯一二级索引，那么两个事务，前后执行相同的 Insert 语句，是不会发生阻塞的

### 3、如何避免死锁

死锁的四个必要条件：**互斥、占有且等待、不可强占用、循环等待**。只要系统发生死锁，这些条件必然成立，但是只要破坏任意一个条件就死锁就不会成立。 

#### 3.1、设置事务等待锁的超时时间

当一个事务的等待时间超过该值后，就对这个事务进行回滚，于是锁就释放了，另一个事务就可以继续执行了。在 InnoDB 中，参数 `innodb_lock_wait_timeout` 是用来设置超时时间的，默认值时 50 秒。 

#### 3.2、开启主动死锁检测

主动死锁检测在发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 on，表示开启这个逻辑，默认就开启。 

#### 3.3、预防死锁

上两个都是**死锁发生**后的解决方式。可以回归到业务的角度**预防死锁**。

对订单做幂等性校验的目的是为了保证不会出现重复的订单，那我们可以直接将 order_no 字段设置为**唯一索引列**，利用它的唯一性来保证订单表不会出现重复的订单，这样就能避免加锁，但是一旦插入已经存在的订单记录就会抛出异常。

### 4、死锁案例

#### ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E9%94%81/%E5%AD%97%E8%8A%82mysql%E9%9D%A2%E8%AF%95%E9%A2%98.png) 4.1、T1分析

共加了两个锁，分别是：

- 表锁：X 类型的意向锁；
- 行锁：X 类型的间隙锁；

重点关注行锁，此时事务 A 在**主键索引**上加的是**间隙锁，锁范围是`(20, 30)`**。 

> 锁范围如何确认？
>
> 如果 LOCK_MODE 是 next-key 锁或者间隙锁，那么 LOCK_DATA 就表示**锁的范围最右值**，此次的事务 A 的 LOCK_DATA 是 30。 
>
> 锁范围的最左值是 t_student 表中 **id 为 30 的上一条记录的 id 值**，即 20。 

#### 4.2、T2分析

B事物在T2的操作与A事物在T1的操作一样，**加的是间隙锁，锁范围是`(20, 30)`**。 

两个事务的间**隙锁之间是相互兼容**的，不会产生冲突。 

**间隙锁的意义只在于阻止区间被插入**，因此是可以共存的。**一个事务获取的间隙锁不会阻止另一个事务获取同一个间隙范围的间隙锁**，共享（S型）和排他（X型）的间隙锁是没有区别的，他们相互不冲突，且功能相同。 

#### 4.3、T3分析

事务 A 的状态为等待状态，因为向事务 B 生成的间隙锁（范围 `(20, 30)`）中插入了一条记录，所以事务 A 的插入操作生成了一个**插入意向锁**。 

**插入意向锁是一种特殊的间隙锁，但不同于间隙锁的是，该锁只用于并发插入操作**。

**尽管「插入意向锁」也属于间隙锁，但两个事务却不能在同一时间内，一个拥有间隙锁，另一个拥有该间隙区间内的插入意向锁（当然，插入意向锁如果不在间隙锁区间内则是可以的）。所以，插入意向锁和间隙锁之间是冲突的**。 

> 插入意向锁的生成时机？
>
> 每插入一条新记录，都需要看一下**待插入记录的下一条记录上是否已经被加了间隙锁**，如果已加间隙锁，此时会生成一个插入意向锁，然后锁的状态设置为等待状态，现象就是 Insert 语句会被阻塞。 

#### 4.4、T4分析

事务 B 在生成插入意向锁时而导致被阻塞，这是因为事务 B 向事务 A 生成的范围为 (20, 30) 的间隙锁插入了一条记录，而插入意向锁和间隙锁是冲突的，所以事务 B 在获取插入意向锁时就陷入了等待状态。 

#### 4.5、死锁原因

本次案例中，事务 A 和事务 B 在执行完后 update 语句后都持有范围为`(20, 30）`的间隙锁，而接下来的插入操作为了获取到插入意向锁，都在等待对方事务的间隙锁释放，于是就造成了循环等待，满足了死锁的四个条件：**互斥、占有且等待、不可强占用、循环等待**，因此发生了死锁。 

### 5、总结

两个事务即使生成的**间隙锁的范围是一样的，也不会发生冲突**，因为间隙锁目的是为了防止其他事务插入数据，因此间隙锁与间隙锁之间是相互兼容的。

在执行插入语句时，如果插入的记录在其他事务持有间隙锁范围内，插入语句就会被阻塞，因为插入语句在碰到间隙锁时，会生成一个插入意向锁，然后**插入意向锁和间隙锁之间是互斥的关系**。

如果**两个事务分别向对方持有的间隙锁范围内插入一条记**录，而插入操作为了获取到插入意向锁，都在等待对方事务的间隙锁释放，于是就造成了循环等待，满足了死锁的四个条件：**互斥、占有且等待、不可强占用、循环等待**，因此发生了死锁。



## 二十、解决幻读

### 1、幻读

同一个查询在不同的时间产生不同的结果集时，事务中就会出现所谓的幻象问题。例如，如果 SELECT 执行了两次，但第二次返回了第一次没有返回的行，则该行是“幻像”行。

### 2、MySQL解决幻读的方式

MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它**很大程度上避免幻读**现象，解决的方案有两种：

- 针对**快照读**（普通 select 语句），是**通过 MVCC 方式解决了幻读**，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。
- 针对**当前读**（select ... for update 等语句），是**通过 next-key lock（记录锁+间隙锁）方式解决了幻读**，因为当执行 select ... for update 语句的时候，会加上 **next-key lock**，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。

### 3、增删改时的索引问题

有一点要注意的是，在执行 update、delete、select ... for update 等具有加锁性质的语句，一定要检查语句是否走了索引，如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住了。



## 二十一、日志

### 1、undo log-回滚日志-原子性

#### 1.1、为什么需要undo log

如果事务执行过程中，都记录下回滚时需要的信息，那么在事务执行中途发生了 MySQL 崩溃后，就可以回滚到事务之前的数据，

##### 1.1.1、保证事务原子性

实现这一机制就是 **undo log（回滚日志），它保证了事务的 ACID 特性中的原子性（Atomicity）**。undo log 是一种用于撤销回退的日志。在事务没提交之前，MySQL 会先记录更新前的数据到 undo log 日志文件里面，当事务回滚时，可以利用 undo log 来进行回滚。 

##### 1.1.2、 **实现 MVCC（多版本并发控制）**

对于「读提交」和「可重复读」隔离级别的事务来说，它们的快照读（普通 select 语句）是通过 Read View + undo log 来实现的，它们的区别在于创建 Read View 的时机不同：

- 「读提交」隔离级别是在**每个 select** 都会生成一个新的 Read View，也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。
- 「可重复读」隔离级别是**启动事务时生成一个 Read View**，然后整个事务期间都在用这个 Read View，这样就保证了在事务期间读到的数据都是事务启动前的记录。

这两个隔离级别实现是通过「**事务的 Read View 里的字段**」和「**记录中的两个隐藏列（trx_id 和 roll_pointer）**」的比对，如果不满足可见行，就会顺着 **undo log 版本链**里找到满足其可见性的记录，从而控制并发事务访问同一个记录时的行为。

因此，undo log 两大作用：

- **实现事务回滚，保障事务的原子性**。事务处理过程中，如果出现了错误或者用户执 行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。
- **实现 MVCC（多版本并发控制）关键因素之一**。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。

#### 1.2、回滚日志的作用

##### 1.2.1、**实现 MVCC（多版本并发控制）**

MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。

##### 1.2.2、实现事务回滚，保证事务原子性

事务处理过程中，如果出现了错误或者用户执行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。

#### 1.3、undo log的实现-版本链

一条记录的每一次更新操作产生的 undo log 格式都有一个 roll_pointer 指针和一个 trx_id 事务id：

- 通过 trx_id 可以知道该记录是**被哪个事务修改**的；
- 通过 roll_pointer 指针可以将这些 undo log **串成一个链表**，这个链表就被称为版本链；

版本链如下图：

![版本链](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/%E7%89%88%E6%9C%AC%E9%93%BE.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)



### 2、Buffer Pool

MySQL数据都保存在磁盘中，全从磁盘读取效率低，可以考虑设计缓存。Innodb 存储引擎设计了一个**缓冲池（Buffer Pool）**，来提高数据库的读写性能。 

有了 Buffer Poo 后：

- 当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。
- 当修改数据时，如果数据存在于 Buffer Pool 中，那直接修改 Buffer Pool 中数据所在的页，然后将其页设置为**脏页**，为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。

#### 2.1、Buffer Pool缓存什么

InnoDB 会把存储的数据划分为若干个「页」，以页作为磁盘和内存交互的基本单位，一个页的默认大小为 16KB。因此，**Buffer Pool 同样需要按「页」来划分**。

在 MySQL 启动的时候，**InnoDB 会为 Buffer Pool 申请一片连续的内存空间，然后按照默认的`16KB`的大小划分出一个个的页， Buffer Pool 中的页就叫做缓存页**。此时这些缓存页都是空闲的，之后随着程序的运行，才会有磁盘上的页被缓存到 Buffer Pool 中。

> **MySQL 刚启动**的时候，会观察到**使用的虚拟内存空间很大，而使用到的物理内存空间却很小**，这是因为只有这些虚拟内存被访问后，操作系统才会触发**缺页中断**，申请物理内存，接着将虚拟地址和物理地址建立映射关系。 

Buffer Pool 除了缓存「**索引页**」和「**数据页**」，还包括了 **Undo 页，插入缓存、自适应哈希索引、锁信息**等等。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/bufferpool%E5%86%85%E5%AE%B9.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)

#### 2.2、Undo 页是记录什么？ 

开启事务后，InnoDB 层更新记录前，首先要记录相应的 undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。 

#### 2.3、 查询一条记录，就只需要缓冲一条记录吗？ 

不是的。当我们查询一条记录时，InnoDB 是会把**整个页**的数据加载到 Buffer Pool 中，将页加载到 Buffer Pool 后，再通过页里的「页目录」去定位到某条具体的记录。

### 3、redo log-重做日志-持久性

为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来，**这个时候更新就算完成了**。

后续，InnoDB 引擎会在适当的时候，由后台线程将缓存在 Buffer Pool 的脏页刷新到磁盘里，这就是 **WAL （Write-Ahead Logging）技术**。

 ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/wal.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0) 

**WAL 技术指的是， MySQL 的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写到磁盘上**。

#### 3.1、什么是redo log

redo log 是物理日志，记录了某个数据页做了什么**修改**，比如**对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新**，每当执行一个事务就会产生这样的一条或者多条物理日志。

在事务提交时，只要先将 redo log 持久化到磁盘即可，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。

当系统崩溃时，虽然脏页数据没有持久化，但是 **redo log 已经持久化**，接着 MySQL 重启后，可以根据 redo log 的内容，将所有数据恢复到最新的状态。 

#### 3.2、undo页面修改，需要记录对应的redo log

开启事务后，InnoDB 层更新记录前，首先要记录相应的 undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。

不过，**在内存修改该 Undo 页面后，也需要记录对应的 redo log**。

#### 3.3、redo log 和 undo log的区别

- redo log 记录了此次事务「**完成后**」的数据状态，记录的是更新**之后**的值；
- undo log 记录了此次事务「**开始前**」的数据状态，记录的是更新**之前**的值；

事务**提交之前**发生了崩溃，重启后会通过 **undo log** **回滚事务**，事务**提交之后**发生了崩溃，重启后会通过 **redo log 恢复事务**。

![事务恢复](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/%E4%BA%8B%E5%8A%A1%E6%81%A2%E5%A4%8D.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)

所以有了 redo log，再通过 WAL 技术，InnoDB 就可以保证即使数据库发生异常重启，之前已提交的记录都不会丢失，这个能力称为 **crash-safe**（**崩溃恢复**）。可以看出来， **redo log 保证了事务四大特性中的持久性**。

#### 3.4、redo log 也要写磁盘，为什么效率高？

写入 redo log 的方式使用了追加操作， 所以磁盘操作是**顺序写**，而写入数据需要先找到写入位置，然后才写到磁盘，所以磁盘操作是**随机写**。

磁盘的「顺序写 」比「随机写」 高效的多，因此 redo log 写入磁盘的开销更小。可以说这是 WAL 技术的另外一个优点：**MySQL 的写操作从磁盘的「随机写」变成了「顺序写」**，提升语句的执行性能。这是因为 MySQL 的写操作并不是立刻更新到磁盘上，而是先记录在日志上，然后在合适的时间再更新到磁盘上 。 

#### 3.5、redo log的优点

- **实现事务的持久性，让 MySQL 有 crash-safe 崩溃恢复的能力**，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失；
- **将写操作从「随机写」变成了「顺序写」**，提升 MySQL 写入磁盘的性能。

#### 3.6、redo log buffer

执行一个事务的过程中，产生的 **redo log 也不是直接写入磁盘的**，因为这样会产生大量的 I/O 操作，而且磁盘的运行速度远慢于内存。

所以，redo log 也有自己的缓存—— **redo log buffer**，每当产生一条 redo log 时，会先写入到 redo log buffer，后续在持久化到磁盘。

redo log buffer 默认大小 16 MB，可以通过 `innodb_log_Buffer_size` 参数动态的调整大小，增大它的大小可以让 MySQL 处理「大事务」是不必写入磁盘，进而提升写 IO 性能 

#### 3.7、redo log什么时候刷盘

主要有下面几个时机：

- MySQL 正常关闭时；
- 当 **redo log buffer** 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘；
- InnoDB 的后台线程**每隔 1 秒**，将 redo log buffer 持久化到磁盘。
- 每次**事务提交时**都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘（这个策略可由 innodb_flush_log_at_trx_commit 参数控制）。

##### 3.7.1、innodb_flush_log_at_trx_commit 

- 当设置该**参数为 0 时**，表示每次事务提交时 ，还是**将 redo log 留在 redo log buffer 中** ，该模式下在事务提交时不会主动触发写入磁盘的操作。

  通过调用 `write()` 写到操作系统的 Page Cache，然后调用 `fsync()` 持久化到磁盘。**所以参数为 0 的策略，MySQL 进程的崩溃会导致上一秒钟所有事务数据的丢失**; 

- 默认值，当设置该**参数为 1 时**，表示每次事务提交时，都**将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘**，这样可以保证 MySQL 异常重启之后数据不会丢失。

- 当设置该**参数为 2 时**，表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log **写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘**，因为操作系统的文件系统中有个 Page Cache，Page Cache 是专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存。

  **所以参数为 2 的策略，较取值为 0 情况下更安全，因为 MySQL 进程的崩溃并不会丢失数据，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失**。 

  ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/innodb_flush_log_at_trx_commit.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0) 

##### 3.7.2、应用场景对比

这三个参数的数据安全性和写入性能的比较如下：

- 数据安全性：参数 1 > 参数 2 > 参数 0
- 写入性能：参数 0 > 参数 2> 参数 1

所以，数据安全性和写入性能是熊掌不可得兼的，**要不追求数据安全性，牺牲性能；要不追求性能，牺牲数据安全性**。

#### 3.8、redo log 写满了怎么办

redo log 是**循环写**的方式，相当于一个环形，InnoDB 用 write pos 表示 redo log 当前记录写到的位置，用 checkpoint 表示当前要擦除的位置，如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/checkpoint.png)

如果 write pos 追上了 checkpoint，就意味着 **redo log 文件满了**，这时 MySQL 不能再执行新的更新操作，也就是说 MySQL 会被阻塞**，此时**会停下来将 Buffer Pool 中的脏页刷新到磁盘中，然后标记 redo log 哪些记录可以被擦除，接着对旧的 redo log 记录进行擦除，等擦除完旧记录腾出了空间，checkpoint 就会往后移动（图中顺时针），然后 MySQL 恢复正常运行，继续执行新的更新操作。

所以，一次 checkpoint 的过程就是脏页刷新到磁盘中变成干净页，然后标记 redo log 哪些记录可以被覆盖的过程。

### 4、binlog-归档日志-备份恢复，主从复制

binlog 文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作。 

#### 4.1、redo log 和 binlog的区别

#####  4.1.1、适用对象不同：

- binlog 是 MySQL 的 **Server 层**实现的日志，所有存储引擎都可以使用；
- redo log 是 **Innodb 存储引擎**实现的日志；

##### 4.1.2、文件格式不同

- binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下：
  - STATEMENT：每一条**修改数据的 SQL** 都会被记录到 binlog 中（相当于记录了逻辑操作，所以针对这种格式， binlog 可以称为**逻辑日志**），主从复制中 slave 端再根据 SQL 语句重现。但 STATEMENT 有动态函数的问题，比如你用了 uuid 或者 now 这些函数，你在主库上执行的结果并不是你在从库执行的结果，这种随时在变的函数会导致复制的数据不一致；
  - ROW：记录**行数据最终被修改成什么样**了（这种格式的日志，就不能称为逻辑日志了），不会出现 STATEMENT 下动态函数的问题。但 ROW 的缺点是每行数据的变化结果都会被记录，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，而在 STATEMENT 格式下只会记录一个 update 语句而已；
  - MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式；
- redo log 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新；

##### 4.1.3、写入方式不同

- binlog 是**追加写**，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。
- redo log 是**循环写**，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。

##### 4.1.4、用途不同

- binlog 用于**备份恢复、主从复制**；
- redo log 用于掉电等**故障恢复**。

#### 4.2、整个数据库数据删除了，不能用redo log文件恢复

不可以使用 redo log 文件恢复，只能使用 binlog 文件恢复。

因为 redo log 文件是**循环写**，是会边写边擦除日志的，只记录未被刷入磁盘的数据的物理日志，已经刷入磁盘的数据都会从 redo log 文件里擦除。

binlog 文件保存的是**全量的日志**，也就是保存了所有数据变更的情况，理论上只要记录在 binlog 上的数据，都可以恢复，所以如果不小心整个数据库的数据被删除了，得用 binlog 文件恢复数据。

#### 4.3、主从复制

MySQL 的**主从复制依赖于 binlog** ，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。

这个过程一般是**异步**的，也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。

![MySQL 主从复制过程](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E8%BF%87%E7%A8%8B.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)

MySQL 集群的主从复制过程梳理成 3 个阶段：

- **写入 Binlog**：主库写 binlog 日志，提交事务，并更新本地存储数据。
- **同步 Binlog**：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。
- **回放 Binlog**：回放 binlog，并更新存储引擎中的数据。

**主从复制的好处**： 在完成主从复制之后，你就可以在**写数据时只写主库，在读数据时只读从库**，这样即使写请求会锁表或者锁记录，也**不会影响读请求的执行**。 

#### 4.4、从库是越多越好吗

不是的。因为从库数量增加，从库连接上来的 I/O 线程也比较多，**主库也要创建同样多的 log dump 线程来处理复制的请求，对主库资源消耗比较高，同时还受限于主库的网络带宽**。

所以在实际使用中，一个主库一般跟 2～3 个从库（1 套数据库，1 主 2 从 1 备主），这就是**一主多从的 MySQL 集群结构**。

#### 4.5、主从复制模型

- **同步复制**：MySQL 主库提交事务的线程要**等待所有从库的复制成功响应，才返回客户端结果**。这种方式在实际项目中，基本上没法用，原因有两个：一是性能很差，因为要复制到所有节点才返回响应；二是可用性也很差，主库和所有从库任何一个数据库出问题，都会影响业务。
- **异步复制**（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。
- **半同步复制**：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种**半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险**。

#### 4.6、binlog什么时候刷盘

一个事务的 binlog 是不能被拆开的，因此无论这个事务有多大（比如有很多条语句），也要保证一次性写入，如果binlog被拆开，在备库执行时就会被当成多个事务分段执行，破坏了**原子性**。在事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 文件中，并清空 binlog cache。如下图：

![binlog cach](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/binlogcache.drawio.png)

虽然每个线程有自己 binlog cache，但是最终都写到同一个 binlog 文件：

- 图中的 write，指的就是指把日志**写入到 binlog 文件**，但是并没有把数据持久化到磁盘，因为数据还缓存在文件系统的 page cache 里，write 的写入速度还是比较快的，因为不涉及磁盘 I/O。
- 图中的 fsync，才是将**数据持久化到磁盘**的操作，这里就会涉及磁盘 I/O，所以频繁的 fsync 会导致磁盘的 I/O 升高。这里是交给操作系统来完成的，所以只要主机不异常断电，就不会错误。

### 5、update方式-结合三种日志

当**优化器分析出成本最小的执行计划后**，执行器就按照执行计划开始进行更新操作。

具体更新一条记录 `UPDATE t_user SET name = 'xiaolin' WHERE id = 1;` 的流程如下:

1. **执行器**负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1 这一行记录：

   - 如果 id=1 这一行所在的数据页本来就在 **buffer pool** 中，就直接返回给执行器更新；
   - 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。

2. 执行器得到聚簇索引记录后，会看一下**更新前的记录和更新后的记录是否一样**：

   - 如果一样的话就不进行后续更新流程；
   - 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作；

3. **开启事务**， InnoDB 层更新记录前，首先要记录相应的 **undo log**，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，**undo log 会写入 Buffer Pool 中的 Undo 页面，不过在内存修改该 Undo 页面后，需要记录对应的 redo log**。

4. InnoDB 层开始更新记录，会**先更新内存（同时标记为脏页）**，然后**将记录写到 redo log** 里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。这就是 **WAL 技术**，MySQL 的写操作并不是立刻写到磁盘上，而是先写 redo 日志，然后在合适的时间再将修改的行数据写到磁盘上。

5. 至此，一条记录更新完了。

6. 在**一条更新语句执行完成后**，然后开始记录该语句对应的 **binlog**，此时记录的 binlog 会被保存到 **binlog cache**，并没有刷新到硬盘上的 binlog 文件，在**事务提交时**才会统一将该事务运行过程中的所有 **binlog 刷新到硬盘**。

7. 事务提交，剩下的就是「两阶段提交」的事情了。

   **prepare 阶段**：将 redo log 对应的事务状态设置为 prepare，然后将 redo log 刷新到硬盘；

   **commit 阶段**：将 binlog 刷新到磁盘，接着调用引擎的提交事务接口，将 redo log 状态设置为 commit（将事务设置为 commit 状态后，刷入到磁盘 redo log 文件）；

8. 至此，一条更新语句执行完成。

### 6、两阶段提交-解决主从不一致

#### 6.1、为什么两阶段提交

事务提交后，redo log 和 binlog 都要持久化到磁盘，但是这两个是独立的逻辑，可能出现**半成功**的状态，这样就造成**两份日志之间的逻辑不一致**。

举个例子，假设 id = 1 这行数据的字段 name 的值原本是 'jay'，然后执行 `UPDATE t_user SET name = 'xiaolin' WHERE id = 1;` 如果在持久化 redo log 和 binlog 两个日志的过程中，出现了半成功状态，那么就有两种情况：

- **如果在将 redo log 刷入到磁盘之后， MySQL 突然宕机了，而 binlog 还没有来得及写入**。MySQL 重启后，通过 redo log 能将 Buffer Pool 中 id = 1 这行数据的 name 字段恢复到新值 xiaolin，但是 binlog 里面没有记录这条更新语句，在主从架构中，binlog 会被复制到从库，由于 binlog 丢失了这条更新语句，从库的这一行 name 字段是旧值 jay，与主库的值不一致性；
- **如果在将 binlog 刷入到磁盘之后， MySQL 突然宕机了，而 redo log 还没有来得及写入**。由于 redo log 还没写，崩溃恢复以后这个事务无效，所以 id = 1 这行数据的 name 字段还是旧值 jay，而 binlog 里面记录了这条更新语句，在主从架构中，binlog 会被复制到从库，从库执行了这条更新语句，那么这一行 name 字段是新值 xiaolin，与主库的值不一致性；

因为 **redo log 影响主库的数据，binlog 影响从库的数据，所以 redo log 和 binlog 必须保持一致才能保证主从数据一致**。

**MySQL 为了避免出现两份日志之间的逻辑不一致的问题，使用了「两阶段提交」来解决**，两阶段提交其实是分布式事务一致性协议，它可以保证多个逻辑操作**要不全部成功，要不全部失败**，不会出现半成功的状态。 

 **两阶段提交把单个事务的提交拆分成了 2 个阶段，分别是「准备（Prepare）阶段」和「提交（Commit）阶段」**，每个阶段都由协调者（Coordinator）和参与者（Participant）共同完成。 

#### 6.2、两阶段提交实现方式

**将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog**，具体如下：

- **prepare 阶段**：将 XID（内部 XA 事务的 ID） **写入到 redo log**，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘；

- **commit 阶段**：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘。

  接着调用引擎的提交事务接口，将 redo log 状态设置为 commit，此时该状态并不需要持久化到磁盘，只需要 write 到文件系统的 page cache 中就够了，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功；

#### 6.3、两阶段提交解决的异常

问题场景：

![时刻 A 与时刻 B](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%B4%A9%E6%BA%83%E7%82%B9.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)

不管是时刻 A（redo log 已经写入磁盘， binlog 还没写入磁盘），还是时刻 B （redo log 和 binlog 都已经写入磁盘，还没写入 commit 标识）崩溃，**此时的 redo log 都处于 prepare 状态**。

在 MySQL 重启后会按顺序扫描 redo log 文件，碰到处于 prepare 状态的 redo log，就拿着 redo log 中的 XID 去 binlog 查看是否存在此 XID：

- **如果 binlog 中没有当前内部 XA 事务的 XID，说明 redolog 完成刷盘，但是 binlog 还没有刷盘，则回滚事务**。对应时刻 A 崩溃恢复的情况。
- **如果 binlog 中有当前内部 XA 事务的 XID，说明 redolog 和 binlog 都已经完成了刷盘，则提交事务**。对应时刻 B 崩溃恢复的情况。

对于处于 prepare 阶段的 redo log，可以提交事务，也可以回滚事务，这**取决于是否能在 binlog 中查找到与 redo log 相同的 XID**，如果有就提交事务，如果没有就回滚事务。这样就可以保证 redo log 和 binlog 这两份日志的一致性了 。

**两阶段提交是以 binlog 写成功为事务提交成功的标识**，因为 binlog 写成功了，就意味着能在 binlog 中查找到与 redo log 相同的 XID。 

#### 6.4、 为什么处于 prepare 阶段的 redo log 加上完整 binlog，重启就提交事务? 

binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。

所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。

#### 6.5、事务未提交时，redo log会被持久化到磁盘吗？

会的。事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些缓存在 redo log buffer 里的 redo log 也会被「后台线程」每隔一秒一起持久化到磁盘。

也就是说，**事务没提交的时候，redo log 也是可能被持久化到磁盘的**。

> 事务未提交的redo log已经被持久化到磁盘了，此时mySQL重启，数据如何保持一致？
>
> 这时mySQL因为binlog还没有持久化到磁盘，就会回滚操作。

#### 6.6、两阶段提交的问题

- **磁盘 I/O 次数高**：为了避免日志丢失，每个事务提交过程中， 都会**至少调用 2 次刷盘操作**，一次是 redo log 刷盘，一次是 binlog 落盘，所以这会成为性能瓶颈。 

- **锁竞争激烈**：两阶段提交虽然能够保证「单事务」两个日志的内容一致，但在「多事务」的情况下，却不能保证两者的提交顺序一致。

  因此，在两阶段提交的流程基础上，还需要加一个锁来保证提交的原子性，在一个事务获取到锁时才能进入 prepare 阶段，一直到 commit 阶段结束才能释放锁，下个事务才可以继续进行 prepare 操作。 

##### 6.6.1、组提交

 **MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数**，如果说 10 个事务依次排队刷盘的时间成本是 10，那么将这 10 个事务一次性一起刷盘的时间成本则近似于 1。 

引入了组提交机制后，prepare 阶段不变，只针对 commit 阶段，将 commit 阶段拆分为三个过程：

- **flush 阶段**：多个事务按进入的顺序将 binlog 从 cache 写入文件（不刷盘）；

  > 在MySQL5.7之后可以进一步改进，将redo log刷盘成组延迟到flush阶段，也支持**redo log的组提交**。

  如果在这一步完成后数据库崩溃，由于 binlog 中没有该组事务的记录，所以 **MySQL 会在重启后回滚**该组事务。 

- **sync 阶段**：对 binlog 文件做 fsync 操作（多个事务的 binlog 合并一次刷盘）；

  支持**sync的组提交**。

  如果在这一步完成后数据库崩溃，由于 binlog 中已经有了事务记录，MySQL会在重启后通过 redo log 刷盘的数据**继续进行**事务的提交。 

- **commit 阶段**：各个事务按顺序做 InnoDB commit 操作；

  commit 阶段队列的作用是承接 sync 阶段的事务，完成最后的引擎提交，使得 sync 可以尽早的处理下一组事务，最大化组提交的效率 。

**每个阶段都有一个队列**，每个阶段有锁进行保护，因此保证了事务写入的顺序。对每个阶段引入了队列后，锁就只针对每个队列进行保护，不再锁住提交事务的整个过程，可以看的出来，**锁粒度减小了，这样就使得多个阶段可以并发执行，从而提升效率**。 

### 7、磁盘I/O很高的优化

#### 7.1、延迟binlog刷盘

binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数 ，通过在刷盘前可以等待多个binlog刷盘任务再执行。这样即使MySQL进程挂掉，也不会造成影响，因为**binlog已经写入 page cache中**，只要系统不要宕机就好。

#### 7.2、延迟fsync

将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000），表示每次提交事务都 write，但**累积 N 个事务后才 fsync**，相当于延迟了 binlog 刷盘的时机。但是这样做的风险是，主机掉电时会丢 N 个事务的 binlog 日志。

#### 7.3、写入 redo log文件

表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log **写到 redo log 文件**，因为操作系统的文件系统中有个 Page Cache，专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存，然后交由操作系统控制持久化到磁盘的时机。

但是这样做的风险是，主机掉电的时候会丢数据。



## 二十二、Buffer Pool

Innodb 存储引擎设计了一个**缓冲池（\*Buffer Pool\*）**，来提高数据库的读写性能。 

有了缓冲池后：

- 当**读取数据**时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。
- 当**修改数据**时，首先是修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页，最后由后台线程将脏页写入到磁盘。

### 1、Buffer Pool 有多大？

Buffer Pool 是在 MySQL 启动的时候，向操作系统申请的一片连续的内存空间，默认配置下 Buffer Pool 只有 `128MB` 。

可以通过调整 `innodb_buffer_pool_size` 参数来设置 Buffer Pool 的大小，一般建议设置成可用物理内存的 60%~80%。

### 2、Buffer Pool缓存什么？

Buffer Pool 除了缓存「**索引页**」和「**数据页**」，还包括了 **undo 页**，插入缓存、自适应哈希索引、锁信息等等。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/bufferpool%E5%86%85%E5%AE%B9.drawio.png)

#### 2.1、刚启动的缓存

在 MySQL 启动的时候，**InnoDB 会为 Buffer Pool 申请一片连续的内存空间，然后按照默认的`16KB`的大小划分出一个个的页， Buffer Pool 中的页就叫做缓存页**。此时这些缓存页都是空闲的，之后随着程序的运行，才会有磁盘上的页被缓存到 Buffer Pool 中。 

MySQL 刚启动的时候，你会观察到使用的**虚拟内存空间很大，而使用到的物理内存空间却很小**，这是因为只有这些虚拟内存被访问后，操作系统才会触发**缺页中断**，接着将虚拟地址和物理地址建立映射关系。 

#### 2.2、缓存控制块

InnoDB 为**每一个缓存页都创建了一个控制块**，控制块信息包括「缓存页的表空间、页号、缓存页地址、链表节点」等等。控制块也是占有内存空间的，它是放在 Buffer Pool 的最前面，接着才是缓存页，如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/%E7%BC%93%E5%AD%98%E9%A1%B5.drawio.png)

控制块和缓存页之间灰色部分称为**碎片空间**。  剩余的那点儿空间不够一对控制块和缓存页的大小，这个用不到的内存空间就被称为碎片。 

#### 2.3、 查询一条记录，就只需要缓冲一条记录吗？ 

不是的。当我们查询一条记录时，InnoDB 是会把**整个页的数据加载到 Buffer Pool 中**，因为，通过**索引只能定位到磁盘中的页，而不能定位到页中的一条记录**。将页加载到 Buffer Pool 后，再通过页里的页目录去定位到某条具体的记录。

### 3、Buffer Pool 管理

#### 3.1、空闲页管理

Buffer Pool是一片连续的内存空间，为了能够快速找到空闲的缓存页，可以使用链表结构，将空闲缓存页的「控制块」作为链表的节点，这个链表称为 **Free 链表**（空闲链表）。 

 ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/freelist.drawio.png) 

##### Free链表内容

Free 链表上除了有**控制块**，还有一个**头节点**，该**头节点包含链表的头节点地址，尾节点地址，以及当前链表中节点的数量等信息**。

Free 链表节点是一个一个的控制块，而每个控制块包含着对应缓存页的地址，所以相当于 **Free 链表节点都对应一个空闲的缓存页**。

##### 取出空闲页方式

有了 Free 链表后，每当需要从磁盘中加载一个页到 Buffer Pool 中时，就从 Free链表中取一个空闲的缓存页，并且把该缓存页对应的控制块的信息填上，然后把该缓存页对应的控制块从 Free 链表中移除。

#### 3.2、脏页管理

更新数据的时候，不需要每次都要写入磁盘，而是将 Buffer Pool 对应的缓存页标记为**脏页**，然后再由后台线程将脏页写入到磁盘。 

##### Flush链表

链表的节点也是控制块，区别在于 Flush 链表的元素都是脏页。 

有了 Flush 链表后，后台线程就可以遍历 Flush 链表，将脏页写入到磁盘。 

#### 3.3、提高缓存命中率

##### 3.3.1、LRU算法

链表头部的节点是最近使用的，而链表末尾的节点是最久没被使用的。那么，当空间不够了，就淘汰**最久没被使用的节点**，从而腾出空间。

简单的 LRU 算法的实现思路是这样的：

- 当**访问**的页在 Buffer Pool 里，就直接把该页对应的 LRU 链表节点移**动到链表的头部**。
- 当访问的页不在 Buffer Pool 里，除了要把页放入到 LRU 链表的头部，如果LRU链表已经满了，还要**淘汰 LRU 链表末尾的节点**。

简单的 LRU 算法无法避免下面这两个问题：

- 预读失效；
- Buffer Pool 污染；

##### 3.3.2、预读失效

**MySQL的预读机制**：根据局部性原理，MySQL 在加载数据页时，会提前把它相邻的数据页一并加载进来，目的是为了减少磁盘 IO。

但是可能这些**被提前加载进来的数据页，并没有被访问**，相当于这个预读是白做了，这个就是**预读失效**。

如果使用简单的 LRU 算法，**这些预读页如果一直不会被访问到**，就会出现一个很奇怪的问题，**不会被访问的预读页却占用了 LRU 链表前排的位置**，而末尾淘汰的页，可能是频繁访问的页，这样就大大降低了缓存命中率。

##### 3.3.3、如何改进预读失效问题

让预读的页停留在 Buffer Pool 里的时间要尽可能的短，**让真正被访问的页才移动到 LRU 链表的头部**，从而保证真正被读取的热数据留在 Buffer Pool 里的时间尽可能长。 

改进了 LRU 算法，将 LRU 划分了 2 个区域：**old 区域 和 young 区域**。young 区域在 LRU 链表的前半部分，old 区域则是在后半部分，如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/young%2Bold.png)

old 区域占整个 LRU 链表长度的比例可以通过 `innodb_old_blocks_pct` 参数来设置，默认是 37，代表整个 LRU 链表中 young 区域与 old 区域比例是 63:37。

**划分这两个区域后，预读的页就只需要加入到 old 区域的头部，当页被真正访问的时候，才将页插入 young 区域的头部**。如果预读的页一直没有被访问，就会从 old 区域移除，这样就不会影响 young 区域中的热点数据。

##### 3.3.4、Buffer Pool污染

当某一个 SQL 语句**扫描了大量的数据**时，在 Buffer Pool 空间比较有限的情况下，可能会将 **Buffer Pool 里的所有页都替换出去，导致大量热数据被淘汰了**，等这些热数据又被再次访问的时候，由于缓存未命中，就会产生大量的磁盘 IO，MySQL 性能就会急剧下降，这个过程被称为 **Buffer Pool 污染**。 

> 即使查询出来的结果集很小，也会造成 Buffer Pool 污染。
>
> 比如，在一个数据量非常大的表，执行了这条语句：
>
> ```sql
> select * from t_user where name like "%xiaolin%";
> ```
>
> 可能这个查询出来的结果就几条记录，但是由于这条语句会发生**索引失效**，所以这个查询过程是**全表扫描**的，接着会发生如下的过程：
>
> - 从磁盘读到的页加入到 LRU 链表的 **old 区域头部**；
> - 当从页里读取行记录时，也就是**页被访问**的时候，就要将该页放到 **young 区域头部**；
> - 接下来拿行记录的 name 字段和字符串 xiaolin 进行模糊匹配，如果符合条件，就加入到结果集里；
> - 如此往复，直到扫描完表中的所有记录。
>
> 经过这一番折腾，原本 **young 区域的热点数据都会被替换掉**。 

##### 3.3.5、如何改进Buffer Pool污染

LRU 链表中 young 区域就是热点数据，只要我们**提高进入到 young 区域的门槛**，就能有效地保证 young 区域里的热点数据不会被替换掉。 

进入到 young 区域条件增加了一个**停留在 old 区域的时间判断**。在对某个处在 old 区域的缓存页进行第一次访问时，就在它对应的控制块中记录下来这个访问时间：

- 如果后续的访问时间与第一次访问的时间**在某个时间间隔内**，那么**该缓存页就不会被从 old 区域移动到 young 区域的头部**；
- 如果后续的访问时间与第一次访问的时间**不在某个时间间隔内**，那么**该缓存页移动到 young 区域的头部**；

这个间隔时间是由 `innodb_old_blocks_time` 控制的，默认是 1000 ms。也就说，**只有同时满足「被访问」与「在 old 区域停留时间超过 1 秒」两个条件，才会被插入到 young 区域头部**，这样就解决了 Buffer Pool 污染的问题。

> 另外，MySQL 针对 young 区域其实做了一个优化，为了**防止 young 区域节点频繁移动到头部**。young 区域前面 1/4 被访问不会移动到链表头部，只有后面的 3/4被访问了才会。 

#### 3.4、脏页刷盘

引入了 Buffer Pool 后，当修改数据时，首先是修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页，但是磁盘中还是原数据。  **一般都会在一定时机进行批量刷盘 ，改进性能。**

> 如果没来得及刷盘就宕机怎么办？
>
> InnoDB 的更新操作采用的是 **Write Ahead Log** 策略，即**先写日志，再写入磁盘**，通过 **redo log 日志让 MySQL 拥有了崩溃恢复能力**。 

下面几种情况会触发脏页的刷新：

- 当 **redo log 日志满**了的情况下，会主动触发脏页刷新到磁盘；
- **Buffer Pool 空间不足**时，需要将一部分数据页淘汰掉，如果淘汰的是脏页，需要先将脏页同步到磁盘；
- MySQL 认为**空闲**时，后台线程会定期将适量的脏页刷入到磁盘；
- MySQL 正**常关闭**之前，会把所有的脏页刷入到磁盘；

在我们开启了慢 SQL 监控后，如果你发现**「偶尔」会出现一些用时稍长的 SQL**，这可能是因为脏页在刷新到磁盘时可能会给数据库带来性能开销，导致数据库操作抖动。如果间断出现这种现象，就需要调大 **Buffer Pool 空间**或 **redo log 日志**的大小。

### 4、总结

Innodb 存储引擎设计了一个**缓冲池（Buffer Pool）**，来提高数据库的读写性能。

Buffer Pool **以页为单**位缓冲数据，可以通过 `innodb_buffer_pool_size` 参数调整缓冲池的大小，默认是 128 M。

Innodb 通过三种链表来管理缓页：

- Free List （空闲页链表），管理空闲页；
- Flush List （脏页链表），管理脏页；
- LRU List，管理**脏页+干净页**，将最近且经常查询的数据缓存在其中，而不常查询的数据就淘汰出去。；

InnoDB 对 LRU 做了一些优化，我们熟悉的 LRU 算法通常是将最近查询的数据放到 LRU 链表的头部，而 InnoDB 做 3 点优化：

- 将 LRU 链表 分为**young 和 old 两个区域**，加入缓冲池的页，优先插入 old 区域；页被访问时，才进入 young 区域，目的是为了解决**预读失效**的问题。
- 当**「页被访问」且「 old 区域停留时间超过 `innodb_old_blocks_time` 阈值（默认为1秒）」**时，才会将页插入到 young 区域，否则还是插入到 old 区域，目的是为了解决**批量数据访问，大量热数据淘汰**的问题。
- 为了防止 young 区域节点频繁移动到头部。young 区域前面 1/4 被访问不会移动到链表头部，只有后面的 3/4被访问了才会。

可以通过调整 `innodb_old_blocks_pct` 参数，设置 young 区域和 old 区域比例。

在开启了慢 SQL 监控后，如果你发现「偶尔」会出现一些用时稍长的 SQL，这可因为脏页在刷新到磁盘时导致数据库性能抖动。如果在很短的时间出现这种现象，就需要调大 Buffer Pool 空间或 redo log 日志的大小。